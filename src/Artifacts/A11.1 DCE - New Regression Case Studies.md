# Artifact A11.1: DCE - New Regression Case Studies
# Date Created: C1
# Author: AI Model & Curator
# Updated on: C48 (Add max_tokens and navigation bug case studies)

- **Key/Value for A0:**
- **Description:** A separate log for new regression case studies to avoid bloating the original A11 artifact.
- **Tags:** bugs, regression, troubleshooting, development, best practices

## 1. Purpose

This document serves as a living record of persistent or complex bugs that have recurred during development. By documenting the root cause analysis (RCA) and the confirmed solution for each issue, we create a "source of truth" that can be referenced to prevent the same mistakes from being reintroduced into the codebase.

**This artifact is the historical archive for older case studies.** New, active issues should be logged in `A11. DCE - Regression Case Studies.md`. This separation keeps the primary document focused and manageable in size.

## 2. Case Studies

---

### Case Study 043: Navigation Fails After View Rollback

-   **Artifacts Affected:** `src/client/views/parallel-copilot.view/view.tsx`, `src/backend/services/history.service.ts`
-   **Cycles Observed:** C47
-   **Symptom:** After the backend successfully generates responses and creates a new cycle, the frontend UI does not navigate to the new cycle and the navigation buttons are unresponsive.
-   **Root Cause Analysis (RCA):** The curator rolled back `view.tsx` to a version from Cycle 43 to fix a separate UI bug. This older version predated the fix implemented in Cycle 44 for this exact navigation issue. The bug is that the frontend's `maxCycle` state is not updated before the navigation is attempted. The navigation logic in `handleCycleChange` checks if the target cycle ID is less than or equal to `maxCycle`. Since `maxCycle` is stale, this check fails, and the navigation is silently aborted.
-   **Codified Solution & Best Practice:** The backend must be the source of truth for state changes. The `createNewCycleWithResponses` method must return the new maximum cycle count. The frontend message handler for `SendBatchGenerationComplete` **must** update its `maxCycle` state *before* attempting to navigate to the new cycle. This ensures the UI state is synchronized before an action that depends on it is triggered.

---

### Case Study 042: API Error 400 Due to `max_tokens` Misunderstanding

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C47
-   **Symptom:** When sending a request to the vLLM server with a large prompt, setting a high `max_tokens` value causes a `400 Bad Request` error.
-   **Root Cause Analysis (RCA):** The `max_tokens` parameter in the OpenAI API does not define the model's total context window; it defines the maximum number of tokens to *generate in the completion*. The total token count of the prompt plus the `max_tokens` value cannot exceed the model's context length. The user, assuming `max_tokens` was the total context, set it to a very large value (e.g., 131,000). When added to a large prompt (e.g., 24,000 tokens), the sum exceeded the model's limit, resulting in a valid API error.
-   **Codified Solution & Best Practice:** Do not hardcode a large, arbitrary `max_tokens` value on the client side. It is better to omit the parameter entirely and let the inference server (vLLM) use its own configured default. The server is better positioned to manage token limits. The client-side `llm.service.ts` was corrected by removing the `max_tokens` parameter from its `fetch` request body.

---

### Case Study 041: UI Input Wiped Due to `useCallback` Dependency Loop
(No change from C47)

---
... (rest of the file remains the same) ...