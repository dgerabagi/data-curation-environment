# Artifact A96: DCE - Harmony-Aligned Response Schema Plan
# Date Created: C44
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** An analysis of the `openai_harmony` library and a proposed plan for migrating the DCE's vLLM interaction schema from XML tags to a more robust, token-based structured format.
- **Tags:** plan, architecture, interaction schema, parsing, llm, vllm, harmony

## 1. Overview & Goal

The current interaction schema (`A52.2`) relies on parsing XML-like tags (`<file>`, `<summary>`) and markdown headers from the LLM's free-text response. While functional, this approach is brittle. It is susceptible to minor formatting errors from the model and requires complex, string-based `stop` tokens that can prematurely truncate responses.

The `GPT-OSS` repository introduces a more advanced approach, "Harmony," which uses a vocabulary of special control tokens (e.g., `<|start|>`, `<|channel|>`, `<|message|>`, `<|end|>`) to guide the model's generation into a structured, machine-readable format. This is a significantly more robust and powerful way to handle structured data generation and tool use with LLMs.

The goal of this plan is to outline a phased migration from our current XML-based schema to a Harmony-aligned schema for all communication with the vLLM backend.

## 2. Analysis of the Harmony Approach

The `openai_harmony` library and `harmony_vllm_app.py` demonstrate a sophisticated workflow:

1.  **Structured Prompt Rendering:** Instead of a single block of text, the prompt is constructed as a series of messages, each with a `role` (system, user, assistant), and potentially a `channel` (analysis, commentary, final) and `recipient` (e.g., a tool like `python` or `browser`). This entire structure is "rendered" into a sequence of tokens that includes the special control tokens.
2.  **Guided Generation:** The model is trained or fine-tuned to understand these control tokens. It learns to "speak" in this format, for example, by placing its internal monologue in an `analysis` channel and its final answer in a `final` channel.
3.  **Robust Parsing:** The response from the model is not just a block of text; it's a stream of tokens that can be parsed deterministically using the same control tokens. A `StreamableParser` can listen to the token stream and identify when the model is opening a new message, writing to a specific channel, or finishing its turn.

This is fundamentally superior to our current regex-based parsing.

## 3. Proposed Migration Plan

This is a major architectural change and should be implemented in phases.

### Phase 1: Adopt Harmony for File Formatting (Immediate)

-   **Goal:** Replace the <file path> and <file_artifact> tags with a Harmony-style equivalent.
-   **New Schema:**
    -   Instead of XML tags, instruct the model to use:
        `<|start_file| path="path/to/file.ts" |>`
        `// file content here`
        `<|end_file|>`
-   **Implementation:**
    -   **`A52.2`:** Update the interaction schema with these new instructions.
    -   **`response-parser.ts`:** Update the `FILE_TAG_REGEX` to use this new format.
    -   **`server.ts`:** The `stop` parameter (if passed from the client) would now be `<|end_file|>`. This is less likely to be generated by accident than `<file_artifact>`.

### Phase 2: Adopt Harmony for Summary/Plan (Future Cycle)

-   **Goal:** Replace `<summary>` and `<course_of_action>` with Harmony channels.
-   **New Schema:**
    -   Instruct the model to place its analysis in one channel and its final plan in another:
        `<|start|><|assistant|><|channel|>analysis<|message|>`
        `My thoughts and analysis...`
        `<|end|>`
        `<|start|><|assistant|><|channel|>final<|message|>`
        `1. Update file A.`
        `2. Update file B.`
        `<|end|>`
-   **Implementation:**
    -   Requires a more significant rewrite of the `response-parser.ts` to understand channels.

### Phase 3: Full Harmony Prompt Rendering (Long-Term)

-   **Goal:** Fully adopt the `openai_harmony` library's approach.
-   **Implementation:**
    -   **`prompt.service.ts`:** Would need a major rewrite to build a `Conversation` object with `Message`s, instead of a single string.
    -   **`server.ts` / `llm.service.ts`:** Would need to call a "render" function (potentially a Python microservice wrapping the Harmony library) to convert the `Conversation` object into the final tokenized prompt to be sent to vLLM.
    -   **`response-parser.ts`:** Would be replaced with a parser that understands the full Harmony token stream.

## 4. Immediate Next Steps

For the next cycle, we will focus on **Phase 1**. This provides an immediate improvement in the robustness of our file parsing without requiring a complete architectural overhaul. It also serves as a good first step to align the AI's output with a more structured, token-based format.