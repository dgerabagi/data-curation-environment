# Artifact A111: DCE - New Regression Case Studies
# Date Created: C99
# Author: AI Model & Curator
# Updated on: C114 (Add Backend SSE Parser case)

## 1. Purpose

This document serves as a living record of persistent or complex bugs. By documenting the root cause analysis (RCA) and the confirmed solution for each issue, we create a "source of truth" to prevent the same mistakes from being reintroduced into the codebase.

## 2. Case Studies

---

### Case Study 019: Backend SSE Parser Fails on Fragmented Data Chunks

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C108, C109, C110, C114
-   **Symptom:** The DCE output log is flooded with `Could not parse JSON object from stream` warnings during response generation. The final response content is often corrupted or incomplete.
-   **Root Cause Analysis (RCA):** The stream consumer in `llm.service.ts` was not correctly buffering incoming data from the vLLM server. It processed each `data` event from the stream as if it contained one or more complete, newline-terminated messages. However, a single logical SSE message from the server can be fragmented and sent across multiple TCP packets, resulting in the `data` event firing with an incomplete piece of a message. The parser would then attempt to parse this incomplete fragment, which is invalid JSON, leading to the error.
-   **Codified Solution & Best Practice:**
    1.  Stream consumers for protocols like SSE must be designed to handle fragmented messages. They cannot assume that each `data` event contains a complete message.
    2.  The correct implementation is to maintain a persistent buffer outside the `stream.on('data')` handler.
    3.  Each incoming chunk should be appended to this buffer.
    4.  The buffer should then be processed in a loop, searching for the standard SSE message terminator (`\n\n`).
    5.  Only the complete messages found before the terminator should be extracted and parsed. Any remaining text after the last terminator is an incomplete message and must be kept in the buffer to be prepended to the next incoming chunk.

---

### Case Study 018: Parallel Fetch Requests Fail with ETIMEDOUT

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C113
-   **Symptom:** When the backend attempts to make multiple parallel `fetch` requests to the same host (e.g., generating 4 responses simultaneously), some of the requests fail with a `connect ETIMEDOUT` error. This results in only one or two responses being generated successfully.
-   **Root Cause Analysis (RCA):** The default Node.js `http.Agent` (used by `node-fetch`) has a pool of sockets for connections. While the default `maxSockets` is technically `Infinity`, in practice, factors within the execution environment (like VS Code's extension host) can lead to exhaustion or delays in acquiring new sockets for concurrent requests to the same origin. When the application tries to open several connections at once, the later requests can time out while waiting for a socket to become available.
-   **Codified Solution & Best Practice:**
    1.  When an application needs to make a high number of concurrent, long-lived HTTP requests to a single host from a Node.js backend, do not rely on the default `http.Agent`.
    2.  Use a dedicated agent library like `agentkeepalive` to create a custom `HttpsAgent` instance.
    3.  Configure this agent with a high `maxSockets` value (e.g., 100) to ensure a large enough connection pool is available.
    4.  Pass this custom agent to all relevant `fetch` calls. This provides robust and performant connection pooling, preventing timeout errors caused by socket exhaustion.

---

### Case Study 017: Parser Appears to Corrupt Newline Sequences

-   **Artifacts Affected:** `src/client/utils/response-parser.ts`
-   **Cycles Observed:** C112, C113
-   **Symptom:** Valid double-newline sequences (`\n\n`) in the raw AI response are being rendered incorrectly in the UI as `n\n`. This indicates that a backslash is being stripped at some point during parsing or sanitization.
-   **Root Cause Analysis (RCA):** The exact cause is unclear, but the primary suspect is an overly aggressive string replacement intended to fix a different model-specific quirk (`n\n`). The logic is likely misinterpreting or incorrectly modifying valid newline escape sequences. A definitive RCA requires observing the data transformation.
-   **Codified Solution & Best Practice:**
    1.  When debugging a string manipulation or parsing issue where the output is corrupted, the first step is to inject logging to trace the data's state at each step of the transformation.
    2.  Add `logger.log` statements in the parser to output the string immediately after it's received and after each significant `replace()` or sanitization operation.
    3.  This "before and after" logging provides an unambiguous view of how the data is being altered, allowing for the precise correction of the faulty logic.

---

### Case Study 016: Parser Fails to Sanitize Inconsistent Newlines from LLM JSON Output

-   **Artifacts Affected:** `src/client/utils/response-parser.ts`
-   **Cycles Observed:** C112
-   **Symptom:** When parsing a JSON response from the LLM, the final file content in the UI is corrupted. It shows extra `n` characters, for example, rendering `wordn\n` instead of `word\n`, or `wordn- list item` instead of `word\n- list item`.
-   **Root Cause Analysis (RCA):** The LLM, when instructed to produce JSON output, is not perfectly consistent in how it represents newlines within string values. While it correctly uses the `\n` escape sequence for most newlines, it sometimes erroneously inserts an extra literal `n` character immediately before the newline (`...wordn\n...`) or before a markdown formatting character (`...wordn- list item...`). The existing parser correctly handled the standard `\n` but did not have a sanitization step to clean up these spurious `n` characters, causing them to be passed through to the final rendered output.
-   **Codified Solution & Best Practice:**
    1.  Parsers for LLM output must be defensive and should include sanitization steps to handle common, minor formatting errors and model-specific quirks.
    2.  After parsing the main structure (e.g., via `JSON.parse`), apply a series of targeted regular expression replacements to the string content to clean up known inconsistencies.
    3.  For this specific issue, add a replacement for the pattern `n\n` -> `\n` to correct the malformed newlines without affecting legitimate uses of the letter 'n' in the text.

---

### Case Study 015: "Parse All" Button Shows Raw Text View

-   **Artifacts Affected:** `src/client/views/parallel-copilot.view/hooks/useTabManagement.ts`
-   **Cycles Observed:** C111
-   **Symptom:** After pasting responses, clicking the "Parse All" button correctly updates the UI to show that it is in parsed mode (e.g., the button text changes to "Un-Parse All"), but the content area for the response tab continues to show the raw text `textarea` instead of the structured, parsed view.
-   **Root Cause Analysis (RCA):** A logic error occurred in the `useTabManagement` hook. The `handleGlobalParseToggle` function, which is triggered by the "Parse All" button, was responsible for setting the `isParsedMode` state to `true`. However, it failed to also call the `parseAllTabs()` function. The UI then attempted to re-render in parsed mode, but the `parsedContent` property on the tab's state object was still `null`. The conditional rendering logic in the `ResponsePane` component correctly saw that `parsedContent` was null and fell back to displaying the raw `textarea`.
-   **Codified Solution & Best Practice:**
    1.  UI actions that change the "mode" of a view must also ensure the data required for that mode is generated.
    2.  The `handleGlobalParseToggle` function must be modified. When the state is transitioning *to* parsed mode (`isParsedMode = true`), it must immediately call the `parseAllTabs()` function to populate the `parsedContent` state for all tabs. This ensures the necessary data is available before the UI re-renders in the new mode.

---

### Case Study 014: Parsed View Does Not Display Selected File Content

-   **Artifacts Affected:** `src/client/views/parallel-copilot.view/view.tsx`
-   **Cycles Observed:** C111
-   **Symptom:** After parsing an AI response, the "Associated Files" list is displayed correctly. However, clicking on a file in this list does not cause the file's content to be displayed in the code viewer pane on the right. The pane remains empty or shows its default "Select a file" message.
-   **Root Cause Analysis (RCA):** This was a state propagation issue introduced during the major refactor into custom hooks. The main container component (`view.tsx`) is responsible for orchestrating the flow of data between hooks and down to presentational components. The logic for deriving the `viewableContent` prop (which is passed to the code viewer) from the `selectedFilePath` state (managed in `useFileManagement.ts`) was lost. The container was no longer re-calculating which content to display when the selected file changed.
-   **Codified Solution & Best Practice:**
    1.  Container components are responsible for deriving props from state managed by different hooks.
    2.  A `useMemo` hook must be re-implemented in the `view.tsx` container component. This hook's responsibility is to calculate the `viewableContent` string.
    3.  Its dependency array must include all relevant state values: `fileManagement.selectedFilePath`, `tabManagement.tabs`, `tabManagement.activeTab`, and `fileManagement.highlightedCodeBlocks`.
    4.  When `selectedFilePath` changes, the `useMemo` hook will re-run, look up the correct content from the appropriate tab's `parsedContent` or the `highlightedCodeBlocks` cache, and update the `viewableContent` variable. This variable is then passed down through props, ensuring the UI updates correctly.

---

### Case Study 013: Tokens/Sec Calculation Fails with Parallel Streams

-   **Artifacts Affected:** `src/client/views/parallel-copilot.view/hooks/usePcppIpc.ts`
-   **Cycles Observed:** C111
-   **Symptom:** During response generation in the progress UI, the "Tokens/sec" metric remains at 0 or `NaN` and does not update, even though tokens are clearly streaming in for multiple responses.
-   **Root Cause Analysis (RCA):** The refactor to a "fan-out" architecture, where `N` parallel streams are processed, broke the simple tokens-per-second calculation. The `UpdateSingleGenerationProgress` IPC handler was receiving progress for one stream at a time and updating its state array, but it lacked the logic to then re-aggregate the data from *all* active streams to calculate a global TPS metric. It was no longer sufficient to look at a single stream's progress.
-   **Codified Solution & Best Practice:**
    1.  When calculating aggregate metrics from multiple asynchronous data sources, the update handler for any individual source must trigger a re-calculation of the aggregate.
    2.  The IPC message handler for `UpdateSingleGenerationProgress` must be enhanced. Inside its `setGenerationProgress` callback, after updating the array with the new progress for a single response, it must perform an aggregation step.
    3.  This step involves iterating over the entire updated progress array to: a) find the earliest `startTime` among all responses that are not yet complete, and b) calculate the sum of `thinkingTokens + currentTokens` across all responses.
    4.  The global TPS can then be calculated (`totalTokens / elapsedTime`) and the `tps` state updated.

---

### Case Study 012: SSE Parser Fails on JSON Chunks Containing Brace Characters

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C110
-   **Symptom:** During response generation, the logs show sporadic `Could not parse JSON object from stream` warnings. The final, assembled response content in the UI is truncated, missing its beginning characters (e.g., missing the leading `{`).
-   **Root Cause Analysis (RCA):** The parser designed to handle concatenated JSON objects in an SSE stream used a naive brace-counting (`{}`) algorithm to find object boundaries. This algorithm failed when the `delta.content` from the vLLM contained brace characters as part of its string value (e.g., `{"content": "{\\"some\\": \\"json\\"}"}`). The parser would incorrectly count the brace inside the string literal and prematurely slice the stream, attempting to parse an incomplete JSON fragment. This resulted in both the parsing error and the loss of the initial content chunks.
-   **Codified Solution & Best Practice:**
    1.  Manual parsers for structured text formats like JSON must account for the language's syntax, such as string literals.
    2.  The brace-counting logic must be enhanced to be "string-aware." The parser should track whether its iterator is currently inside a double-quoted string. Brace characters (`{` and `}`) should only be counted towards object boundary detection when the iterator is *not* inside a string. This ensures that only structural braces are considered, making the parser robust.

---

### Case Study 011: SSE Parser Fails on Concatenated JSON Objects

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C109
-   **Symptom:** The log shows "Could not parse SSE chunk" for what appears to be valid JSON. This happens frequently during the "thinking" phase of generation where many small `reasoning_content` updates are sent. The UI does not update with streaming tokens for this phase.
-   **Root Cause Analysis (RCA):** The SSE stream parser correctly splits messages by newline and removes the `data: ` prefix. However, it assumed the remaining string is a single JSON object. The vLLM server, especially when sending rapid, small updates, can concatenate multiple JSON objects into a single `data:` payload (e.g., `data: {"id":1}{"id":2}`). Attempting to `JSON.parse()` this concatenated string is a syntax error, causing the parser to fail.
-   **Codified Solution & Best Practice:**
    1.  The SSE `data` payload should not be assumed to be a single JSON object.
    2.  Implement a more robust parser that can handle a stream of concatenated JSON objects. A simple and effective method is to iterate through the string, use a counter to track the nesting level of braces (`{}`), and slice out and parse each complete top-level object when its brace count returns to zero. This makes the parser resilient to variations in how the server batches data within SSE messages.

---

### Case Study 010: Could not parse SSE chunk

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C108
-   **Symptom:** When generating responses, the DCE output channel is flooded with warnings: `[WARN] Could not parse SSE chunk: {"id":...}`. No token counts or partial content appear in the UI, and the generation appears to hang from the user's perspective.
-   **Root Cause Analysis (RCA):** After fixing a stream type mismatch in a previous cycle, the stream consumer in `llm.service.ts` was still not correctly parsing the Server-Sent Events (SSE) protocol used by the OpenAI-compatible vLLM endpoint. The `stream.on('data')` handler was receiving raw data chunks but was not processing them according to the SSE format. A single chunk can contain multiple messages, and each message is prefixed with `data: `. The code was likely attempting to `JSON.parse()` the raw chunk, including the prefix, which is invalid JSON and causes the parsing to fail.
-   **Codified Solution & Best Practice:**
    1.  When consuming an SSE stream in Node.js, the `data` event handler must act as a proper SSE client parser.
    2.  The handler must maintain a buffer of incoming data.
    3.  On each `data` event, append the new chunk to the buffer.
    4.  Split the buffer by newline characters (`\n`) to process complete lines. The last, potentially incomplete, line should be kept in the buffer for the next chunk.
    5.  For each complete line, check if it starts with the `data: ` prefix. If so, slice the string to remove the prefix, trim it, and then parse the result as JSON.
    6.  Handle special SSE messages, such as `data: [DONE]`, to correctly terminate the stream.

---

### Case Study 009: Token Counts and Content Do Not Stream in UI

-   **Artifacts Affected:** `llm.service.ts`, `channels.type.ts`, `usePcppIpc.ts`
-   **Cycles Observed:** C107
-   **Symptom:** In the "Generating Responses" UI, the progress bars correctly change status (e.g., to 'thinking', 'generating'), but the token counters remain at zero, and the partial text preview is empty. The UI only updates when the entire response is complete.
-   **Root Cause Analysis (RCA):** The refactor to a "fan-out" request architecture in Cycle 105 introduced a new IPC channel, `UpdateSingleGenerationProgress`. The payload for this message correctly sent metric updates (like the response `status`) but failed to include the accumulating text `content` of the response. The frontend UI relied on this `content` string to derive the live token counts and to display the partial text preview. Without the streaming content, these UI elements could not be updated in real-time.
-   **Codified Solution & Best Practice:**
    1.  When designing IPC messages for streaming data, ensure the payload contains all necessary information for the UI to reconstruct its state, including both metrics and the partial data itself.
    2.  The IPC payload for `UpdateSingleGenerationProgress` must be enhanced to include the partial `content` string: `{ progress: GenerationProgress; content: string; }`.
    3.  The backend stream handler (`llm.service.ts`) must accumulate the content as chunks arrive and include it in every progress message.
    4.  The frontend IPC listener (`usePcppIpc.ts`) must be updated to use this complete payload to update all relevant pieces of state simultaneouslyâ€”in this case, both the `generationProgress` state (for metrics) and the `tabs` state (for content).

---

### Case Study 008: TypeError: The "readableStream" argument must be an instance of ReadableStream

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C106
-   **Symptom:** When generating responses, the backend crashes with the error `TypeError: The "readableStream" argument must be an instance of ReadableStream. Received an instance of PassThrough`. No response data is processed by the extension.
-   **Root Cause Analysis (RCA):** The `_generateSingleStream` function in `llm.service.ts` was incorrectly trying to convert the response body stream from a `node-fetch` call into a Node.js stream using `Readable.fromWeb()`. This function is designed to convert a Web API `ReadableStream` into a Node.js `Readable`. However, the `response.body` from the `node-fetch` library is *already* a Node.js `Readable` stream (in this case, a `PassThrough` stream). Passing a Node.js stream as an argument to `Readable.fromWeb()` results in a `TypeError`.
-   **Codified Solution & Best Practice:**
    1.  Verify the stream types being returned by libraries. `node-fetch` provides a Node.js-compatible stream, not a Web Stream, in a Node.js environment.
    2.  The `Readable.fromWeb()` conversion was unnecessary. The `response.body` object from the `fetch` call should be used directly, as it is already the correct type for attaching Node.js stream event handlers (`.on('data', ...)`).

---

### Case Study 007: Response Progress UI Fails to Render After Fan-Out Refactor

-   **Artifacts Affected:** `src/client/views/parallel-copilot.view/hooks/usePcppIpc.ts`
-   **Cycles Observed:** C105
-   **Symptom:** When a new generation is started, the UI correctly switches to the "Generating Responses..." view, but the `GenerationProgressDisplay` component remains empty. No progress bars or response items ever appear, despite vLLM processing the requests.
-   **Root Cause Analysis (RCA):** This was a state initialization failure. The `generationProgress` state array in the `useGeneration` hook was initialized as an empty array (`[]`). The IPC handler that received `UpdateSingleGenerationProgress` messages was designed to *update* existing items in this array using `findIndex`. Because the array was empty, `findIndex` always returned -1, and the handler lacked an `else` block to `push` the new progress item into the array. Consequently, the state array remained perpetually empty, and the UI had nothing to render.
-   **Codified Solution & Best Practice:**
    1.  State for dynamic lists that are populated by asynchronous events must be properly initialized.
    2.  The `NavigateToNewGeneratingCycle` IPC handler, which is the event that starts the generation process, is the correct place to initialize the `generationProgress` state. It must create a placeholder `GenerationProgress` object for each expected response (e.g., with a `'pending'` status) and set this as the initial state.
    3.  As a defensive measure, state update handlers for array data should be resilient. They should handle the case where an item is not found, either by adding it or logging an error, rather than failing silently.

---

### Case Study 006: "Stop" Button Aborts All Responses Instead of a Single One

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C103, C104
-   **Symptom:** In the response generation UI, clicking the "Stop" button for a single in-progress response (e.g., "Resp 3") immediately stops all other generating responses.
-   **Root Cause Analysis (RCA):** The backend was making a single batch `fetch` request to the vLLM server (with `n > 1`) and managing this single request with a single `AbortController` keyed by the `cycleId`. When a stop request was received for any response within that cycle, it would look up the controller by `cycleId` and abort the entire batch request, terminating the single HTTP stream that carried all the interleaved responses. There is no mechanism in the SSE or HTTP protocol to cancel just one part of an interleaved stream.
-   **Codified Solution & Best Practice:**
    1.  To enable granular cancellation, the backend must initiate multiple, independent requests instead of a single batch request.
    2.  The `generateBatch` method must be refactored to "fan out." It should loop `N` times and create `N` individual `fetch` requests in parallel (e.g., using `Promise.all`), each with `n: 1`.
    3.  Each of these individual requests must have its own `AbortController`, stored in a map with a more granular key that includes both the `cycleId` and the `responseId` (e.g., `"104_3"`).
    4.  The "Stop" request from the client must include the specific `responseId` to be cancelled, allowing the backend to find and abort only the intended `fetch` request, leaving the others running.

---

### Case Study 005: Unhandled 'AbortError' Crashes Express.js Proxy Server

-   **Artifacts Affected:** `A90. AI Ascent - server.ts (Reference).md`
-   **Cycles Observed:** C102
-   **Symptom:** When a client-side "Stop" action aborts a streaming `fetch` request to an Express.js proxy that is piping a response from a downstream service (like vLLM), the entire Node.js process crashes with a `DOMException [AbortError]: This operation was aborted` and an "Unhandled 'error' event".
-   **Root Cause Analysis (RCA):** The proxy server correctly detects the client disconnection via `res.on('close', ...)` and aborts its own `fetch` request to the downstream service. However, aborting an active `fetch` stream causes the underlying `ReadableStream` to emit an `error` event. In the proxy, the code was piping this stream directly to the client response (`nodeStream.pipe(res)`). When the `nodeStream` emitted the `AbortError`, there was no error handler attached to it (`nodeStream.on('error', ...)`), leading to an unhandled exception that crashed the server.
-   **Codified Solution & Best Practice:**
    1.  Any Node.js `ReadableStream` that is being piped and has the potential to be aborted or encounter an error **must** have an error handler attached.
    2.  The correct implementation is to add a `stream.on('error', (error) => { ... });` handler before calling `stream.pipe()`. This handler should check if the error is an expected `AbortError` and handle it gracefully (e.g., by logging an informational message) while throwing or logging any other unexpected errors. This prevents the process from crashing on a controlled cancellation.

---

### Case Study 004: Proxy Server Aborts vLLM Stream Prematurely

-   **Artifacts Affected:** `A90. AI Ascent - server.ts (Reference).md`
-   **Cycles Observed:** C100, C101
-   **Symptom:** When the DCE extension sends a streaming request to the vLLM via the proxy server, the connection is immediately aborted. The proxy logs show "Client closed the connection. Aborting request to vLLM," even though the client is still waiting for a response.
-   **Root Cause Analysis (RCA):** The logic to handle client-side cancellation was implemented by attaching an event listener to the Express.js `request` object (`req.on('close', ...)`). For a standard HTTP request, the `req` object represents the incoming data from the client. Once the request body is fully received, the `req` stream is finished. However, for a *streaming response*, the long-lived connection is represented by the `response` object (`res`). The `req.on('close')` event was firing prematurely because the initial POST request from the client was completing, which the server misinterpreted as the client disconnecting entirely. The correct event to listen for is `res.on('close')`, which fires only when the client that is *receiving* the streamed response actually closes the connection.
-   **Codified Solution & Best Practice:**
    1.  When implementing cancellation logic for a streaming HTTP response in Express.js, the event listener to detect a client disconnection **must** be attached to the `response` (`res`) object.
    2.  The correct implementation is to use `res.on('close', () => { controller.abort(); });`. This ensures the cancellation is only triggered when the downstream client terminates the connection.

---

### Case Study 003: `AbortController` Lifecycle Bug Causes Unstable Cancellations

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C100
-   **Symptom:** When generating responses, the connection is sometimes aborted immediately and unexpectedly. Furthermore, the manual "Stop Generation" button is unreliable. The root cause is a flaw in how the `AbortController` for the `fetch` request is managed.
-   **Root Cause Analysis (RCA):** The logic for managing streaming `fetch` requests in `generateBatch` and `generateSingle` placed the cleanup code (`generationControllers.delete(cycleId)`) in a `finally` block. For a streaming request, the `await fetch()` promise resolves as soon as the headers are received, allowing the code to proceed while the body streams in asynchronously. Consequently, the `finally` block was executing almost immediately, removing the `AbortController` from the tracking map while the stream was still in progress. This made it impossible to manually cancel the request later and could contribute to unstable connections.
-   **Codified Solution & Best Practice:**
    1.  The lifecycle of a resource tied to a stream (like an `AbortController`) must be managed by the stream's own events, not by a `try/finally` block around the initial `fetch` call.
    2.  The `generationControllers.delete(cycleId)` call must be removed from the `finally` block.
    3.  It must be moved into the terminal event handlers for the stream: `stream.on('end', ...)` and `stream.on('error', ...)` as well as into the main `catch` block that would handle a failure of the initial `fetch` itself. This ensures the controller is only deregistered when the operation is definitively complete or has failed.

---

### Case Study 002: "Re-generate" Button is Non-Functional

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C75, C99
-   **Symptom:** Clicking the "Re-generate" button on a response tab correctly updates the UI to show a loading/spinning state, but no request is ever sent to the vLLM server, and no new response is generated.
-   **Root Cause Analysis (RCA):** The backend method responsible for this feature, `llm.service.ts::generateSingle`, was an unimplemented stub. The entire frontend and IPC message-passing infrastructure was correctly wired up to call this function, but the function itself contained only placeholder logic to update the UI state to "generating" and did not contain any `fetch` call to actually initiate a new LLM request.
-   **Codified Solution & Best Practice:**
    1.  Ensure all backend service methods intended for user-facing features are fully implemented and not just stubs.
    2.  The `generateSingle` method must be implemented to mirror the core logic of `generateBatch`, but for a single response (`n: 1`).
    3.  It must create and execute a streaming `fetch` request to the configured LLM endpoint.
    4.  It must be able to process the resulting Server-Sent Events (SSE) stream and send granular progress updates back to the client using a dedicated IPC channel (e.g., `UpdateSingleGenerationProgress`) to avoid disrupting the state of other, non-regenerating responses.
    5.  Upon completion, it must persist the final, complete response and its metrics via the `HistoryService`.

---

### Case Study 001: "Stop Generation" Does Not Cancel vLLM Request

-   **Artifacts Affected:** `A90. AI Ascent - server.ts (Reference).md`
-   **Cycles Observed:** C70, C75, C99
-   **Symptom:** Clicking the "Stop" button in the response generation UI correctly closes the connection from the DCE extension, but the vLLM server continues to process the request, consuming GPU resources unnecessarily.
-   **Root Cause Analysis (RCA):** The architecture involves the DCE extension making a request to a proxy server, which in turn makes a request to the vLLM server. The `AbortController` in the DCE extension's `llm.service.ts` only aborts the initial request (DCE -> Proxy). The proxy server did not have logic to detect this client-side disconnection and propagate the cancellation to its own downstream request (Proxy -> vLLM). The Express.js response object (`res`) emits a `'close'` event when the client disconnects, which can be used to trigger this cancellation.
-   **Codified Solution & Best Practice:**
    1.  When proxying streaming requests, always propagate client cancellation.
    2.  In the Express.js route handler for the proxy, create a new `AbortController` for the downstream `fetch` request.
    3.  Pass the controller's `signal` to the `fetch` options.
    4.  Register an event listener on the client response object: `res.on('close', () => { controller.abort(); });`. This ensures that if the client hangs up, the server immediately aborts the expensive downstream operation, freeing up resources.