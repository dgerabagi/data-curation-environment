# Artifact A111: DCE - New Regression Case Studies
# Date Created: C99
# Author: AI Model & Curator
# Updated on: C110 (Add JSON Brace Counting case)

## 1. Purpose

This document serves as a living record of persistent or complex bugs. By documenting the root cause analysis (RCA) and the confirmed solution for each issue, we create a "source of truth" to prevent the same mistakes from being reintroduced into the codebase.

## 2. Case Studies

---

### Case Study 012: SSE Parser Fails on JSON Chunks Containing Brace Characters

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C110
-   **Symptom:** During response generation, the logs show sporadic `Could not parse JSON object from stream` warnings. The final, assembled response content in the UI is truncated, missing its beginning characters (e.g., missing the leading `{`).
-   **Root Cause Analysis (RCA):** The parser designed to handle concatenated JSON objects in an SSE stream used a naive brace-counting (`{}`) algorithm to find object boundaries. This algorithm failed when the `delta.content` from the vLLM contained brace characters as part of its string value (e.g., `{"content": "{\\"some\\": \\"json\\"}"}`). The parser would incorrectly count the brace inside the string literal and prematurely slice the stream, attempting to parse an incomplete JSON fragment. This resulted in both the parsing error and the loss of the initial content chunks.
-   **Codified Solution & Best Practice:**
    1.  Manual parsers for structured text formats like JSON must account for the language's syntax, such as string literals.
    2.  The brace-counting logic must be enhanced to be "string-aware." The parser should track whether its iterator is currently inside a double-quoted string. Brace characters (`{` and `}`) should only be counted towards object boundary detection when the iterator is *not* inside a string. This ensures that only structural braces are considered, making the parser robust.

---

### Case Study 011: SSE Parser Fails on Concatenated JSON Objects

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C109
-   **Symptom:** The log shows "Could not parse SSE chunk" for what appears to be valid JSON. This happens frequently during the "thinking" phase of generation where many small `reasoning_content` updates are sent. The UI does not update with streaming tokens for this phase.
-   **Root Cause Analysis (RCA):** The SSE stream parser correctly splits messages by newline and removes the `data: ` prefix. However, it assumed the remaining string is a single JSON object. The vLLM server, especially when sending rapid, small updates, can concatenate multiple JSON objects into a single `data:` payload (e.g., `data: {"id":1}{"id":2}`). Attempting to `JSON.parse()` this concatenated string is a syntax error, causing the parser to fail.
-   **Codified Solution & Best Practice:**
    1.  The SSE `data` payload should not be assumed to be a single JSON object.
    2.  Implement a more robust parser that can handle a stream of concatenated JSON objects. A simple and effective method is to iterate through the string, use a counter to track the nesting level of braces (`{}`), and slice out and parse each complete top-level object when its brace count returns to zero. This makes the parser resilient to variations in how the server batches data within SSE messages.

---

### Case Study 010: Could not parse SSE chunk

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C108
-   **Symptom:** When generating responses, the DCE output channel is flooded with warnings: `[WARN] Could not parse SSE chunk: {"id":...}`. No token counts or partial content appear in the UI, and the generation appears to hang from the user's perspective.
-   **Root Cause Analysis (RCA):** After fixing a stream type mismatch in a previous cycle, the stream consumer in `llm.service.ts` was still not correctly parsing the Server-Sent Events (SSE) protocol used by the OpenAI-compatible vLLM endpoint. The `stream.on('data')` handler was receiving raw data chunks but was not processing them according to the SSE format. A single chunk can contain multiple messages, and each message is prefixed with `data: `. The code was likely attempting to `JSON.parse()` the raw chunk, including the prefix, which is invalid JSON and causes the parsing to fail.
-   **Codified Solution & Best Practice:**
    1.  When consuming an SSE stream in Node.js, the `data` event handler must act as a proper SSE client parser.
    2.  The handler must maintain a buffer of incoming data.
    3.  On each `data` event, append the new chunk to the buffer.
    4.  Split the buffer by newline characters (`\n`) to process complete lines. The last, potentially incomplete, line should be kept in the buffer for the next chunk.
    5.  For each complete line, check if it starts with the `data: ` prefix. If so, slice the string to remove the prefix, trim it, and then parse the result as JSON.
    6.  Handle special SSE messages, such as `data: [DONE]`, to correctly terminate the stream.

---

### Case Study 009: Token Counts and Content Do Not Stream in UI

-   **Artifacts Affected:** `llm.service.ts`, `channels.type.ts`, `usePcppIpc.ts`
-   **Cycles Observed:** C107
-   **Symptom:** In the "Generating Responses" UI, the progress bars correctly change status (e.g., to 'thinking', 'generating'), but the token counters remain at zero, and the partial text preview is empty. The UI only updates when the entire response is complete.
-   **Root Cause Analysis (RCA):** The refactor to a "fan-out" request architecture in Cycle 105 introduced a new IPC channel, `UpdateSingleGenerationProgress`. The payload for this message correctly sent metric updates (like the response `status`) but failed to include the accumulating text `content` of the response. The frontend UI relied on this `content` string to derive the live token counts and to display the partial text preview. Without the streaming content, these UI elements could not be updated in real-time.
-   **Codified Solution & Best Practice:**
    1.  When designing IPC messages for streaming data, ensure the payload contains all necessary information for the UI to reconstruct its state, including both metrics and the partial data itself.
    2.  The IPC payload for `UpdateSingleGenerationProgress` must be enhanced to include the partial `content` string: `{ progress: GenerationProgress; content: string; }`.
    3.  The backend stream handler (`llm.service.ts`) must accumulate the content as chunks arrive and include it in every progress message.
    4.  The frontend IPC listener (`usePcppIpc.ts`) must be updated to use this complete payload to update all relevant pieces of state simultaneouslyâ€”in this case, both the `generationProgress` state (for metrics) and the `tabs` state (for content).

---

### Case Study 008: TypeError: The "readableStream" argument must be an instance of ReadableStream

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C106
-   **Symptom:** When generating responses, the backend crashes with the error `TypeError: The "readableStream" argument must be an instance of ReadableStream. Received an instance of PassThrough`. No response data is processed by the extension.
-   **Root Cause Analysis (RCA):** The `_generateSingleStream` function in `llm.service.ts` was incorrectly trying to convert the response body stream from a `node-fetch` call into a Node.js stream using `Readable.fromWeb()`. This function is designed to convert a Web API `ReadableStream` into a Node.js `Readable`. However, the `response.body` from the `node-fetch` library is *already* a Node.js `Readable` stream (in this case, a `PassThrough` stream). Passing a Node.js stream as an argument to `Readable.fromWeb()` results in a `TypeError`.
-   **Codified Solution & Best Practice:**
    1.  Verify the stream types being returned by libraries. `node-fetch` provides a Node.js-compatible stream, not a Web Stream, in a Node.js environment.
    2.  The `Readable.fromWeb()` conversion was unnecessary. The `response.body` object from the `fetch` call should be used directly, as it is already the correct type for attaching Node.js stream event handlers (`.on('data', ...)`).

---

### Case Study 007: Response Progress UI Fails to Render After Fan-Out Refactor

-   **Artifacts Affected:** `src/client/views/parallel-copilot.view/hooks/usePcppIpc.ts`
-   **Cycles Observed:** C105
-   **Symptom:** When a new generation is started, the UI correctly switches to the "Generating Responses..." view, but the `GenerationProgressDisplay` component remains empty. No progress bars or response items ever appear, despite vLLM processing the requests.
-   **Root Cause Analysis (RCA):** This was a state initialization failure. The `generationProgress` state array in the `useGeneration` hook was initialized as an empty array (`[]`). The IPC handler that received `UpdateSingleGenerationProgress` messages was designed to *update* existing items in this array using `findIndex`. Because the array was empty, `findIndex` always returned -1, and the handler lacked an `else` block to `push` the new progress item into the array. Consequently, the state array remained perpetually empty, and the UI had nothing to render.
-   **Codified Solution & Best Practice:**
    1.  State for dynamic lists that are populated by asynchronous events must be properly initialized.
    2.  The `NavigateToNewGeneratingCycle` IPC handler, which is the event that starts the generation process, is the correct place to initialize the `generationProgress` state. It must create a placeholder `GenerationProgress` object for each expected response (e.g., with a `'pending'` status) and set this as the initial state.
    3.  As a defensive measure, state update handlers for array data should be resilient. They should handle the case where an item is not found, either by adding it or logging an error, rather than failing silently.

---

### Case Study 006: "Stop" Button Aborts All Responses Instead of a Single One

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C103, C104
-   **Symptom:** In the response generation UI, clicking the "Stop" button for a single in-progress response (e.g., "Resp 3") immediately stops all other generating responses.
-   **Root Cause Analysis (RCA):** The backend was making a single batch `fetch` request to the vLLM server (with `n > 1`) and managing this single request with a single `AbortController` keyed by the `cycleId`. When a stop request was received for any response within that cycle, it would look up the controller by `cycleId` and abort the entire batch request, terminating the single HTTP stream that carried all the interleaved responses. There is no mechanism in the SSE or HTTP protocol to cancel just one part of an interleaved stream.
-   **Codified Solution & Best Practice:**
    1.  To enable granular cancellation, the backend must initiate multiple, independent requests instead of a single batch request.
    2.  The `generateBatch` method must be refactored to "fan out." It should loop `N` times and create `N` individual `fetch` requests in parallel (e.g., using `Promise.all`), each with `n: 1`.
    3.  Each of these individual requests must have its own `AbortController`, stored in a map with a more granular key that includes both the `cycleId` and the `responseId` (e.g., `"104_3"`).
    4.  The "Stop" request from the client must include the specific `responseId` to be cancelled, allowing the backend to find and abort only the intended `fetch` request, leaving the others running.

---

### Case Study 005: Unhandled 'AbortError' Crashes Express.js Proxy Server

-   **Artifacts Affected:** `A90. AI Ascent - server.ts (Reference).md`
-   **Cycles Observed:** C102
-   **Symptom:** When a client-side "Stop" action aborts a streaming `fetch` request to an Express.js proxy that is piping a response from a downstream service (like vLLM), the entire Node.js process crashes with a `DOMException [AbortError]: This operation was aborted` and an "Unhandled 'error' event".
-   **Root Cause Analysis (RCA):** The proxy server correctly detects the client disconnection via `res.on('close', ...)` and aborts its own `fetch` request to the downstream service. However, aborting an active `fetch` stream causes the underlying `ReadableStream` to emit an `error` event. In the proxy, the code was piping this stream directly to the client response (`nodeStream.pipe(res)`). When the `nodeStream` emitted the `AbortError`, there was no error handler attached to it (`nodeStream.on('error', ...)`), leading to an unhandled exception that crashed the server.
-   **Codified Solution & Best Practice:**
    1.  Any Node.js `ReadableStream` that is being piped and has the potential to be aborted or encounter an error **must** have an error handler attached.
    2.  The correct implementation is to add a `stream.on('error', (error) => { ... });` handler before calling `stream.pipe()`. This handler should check if the error is an expected `AbortError` and handle it gracefully (e.g., by logging an informational message) while throwing or logging any other unexpected errors. This prevents the process from crashing on a controlled cancellation.

---

### Case Study 004: Proxy Server Aborts vLLM Stream Prematurely

-   **Artifacts Affected:** `A90. AI Ascent - server.ts (Reference).md`
-   **Cycles Observed:** C100, C101
-   **Symptom:** When the DCE extension sends a streaming request to the vLLM via the proxy server, the connection is immediately aborted. The proxy logs show "Client closed the connection. Aborting request to vLLM," even though the client is still waiting for a response.
-   **Root Cause Analysis (RCA):** The logic to handle client-side cancellation was implemented by attaching an event listener to the Express.js `request` object (`req.on('close', ...)`). For a standard HTTP request, the `req` object represents the incoming data from the client. Once the request body is fully received, the `req` stream is finished. However, for a *streaming response*, the long-lived connection is represented by the `response` object (`res`). The `req.on('close')` event was firing prematurely because the initial POST request from the client was completing, which the server misinterpreted as the client disconnecting entirely. The correct event to listen for is `res.on('close')`, which fires only when the client that is *receiving* the streamed response actually closes the connection.
-   **Codified Solution & Best Practice:**
    1.  When implementing cancellation logic for a streaming HTTP response in Express.js, the event listener to detect a client disconnection **must** be attached to the `response` (`res`) object.
    2.  The correct implementation is to use `res.on('close', () => { controller.abort(); });`. This ensures the cancellation is only triggered when the downstream client terminates the connection.

---

### Case Study 003: `AbortController` Lifecycle Bug Causes Unstable Cancellations

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C100
-   **Symptom:** When generating responses, the connection is sometimes aborted immediately and unexpectedly. Furthermore, the manual "Stop Generation" button is unreliable. The root cause is a flaw in how the `AbortController` for the `fetch` request is managed.
-   **Root Cause Analysis (RCA):** The logic for managing streaming `fetch` requests in `generateBatch` and `generateSingle` placed the cleanup code (`generationControllers.delete(cycleId)`) in a `finally` block. For a streaming request, the `await fetch()` promise resolves as soon as the headers are received, allowing the code to proceed while the body streams in asynchronously. Consequently, the `finally` block was executing almost immediately, removing the `AbortController` from the tracking map while the stream was still in progress. This made it impossible to manually cancel the request later and could contribute to unstable connections.
-   **Codified Solution & Best Practice:**
    1.  The lifecycle of a resource tied to a stream (like an `AbortController`) must be managed by the stream's own events, not by a `try/finally` block around the initial `fetch` call.
    2.  The `generationControllers.delete(cycleId)` call must be removed from the `finally` block.
    3.  It must be moved into the terminal event handlers for the stream: `stream.on('end', ...)` and `stream.on('error', ...)` as well as into the main `catch` block that would handle a failure of the initial `fetch` itself. This ensures the controller is only deregistered when the operation is definitively complete or has failed.

---

### Case Study 002: "Re-generate" Button is Non-Functional

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C75, C99
-   **Symptom:** Clicking the "Re-generate" button on a response tab correctly updates the UI to show a loading/spinning state, but no request is ever sent to the vLLM server, and no new response is generated.
-   **Root Cause Analysis (RCA):** The backend method responsible for this feature, `llm.service.ts::generateSingle`, was an unimplemented stub. The entire frontend and IPC message-passing infrastructure was correctly wired up to call this function, but the function itself contained only placeholder logic to update the UI state to "generating" and did not contain any `fetch` call to actually initiate a new LLM request.
-   **Codified Solution & Best Practice:**
    1.  Ensure all backend service methods intended for user-facing features are fully implemented and not just stubs.
    2.  The `generateSingle` method must be implemented to mirror the core logic of `generateBatch`, but for a single response (`n: 1`).
    3.  It must create and execute a streaming `fetch` request to the configured LLM endpoint.
    4.  It must be able to process the resulting Server-Sent Events (SSE) stream and send granular progress updates back to the client using a dedicated IPC channel (e.g., `UpdateSingleGenerationProgress`) to avoid disrupting the state of other, non-regenerating responses.
    5.  Upon completion, it must persist the final, complete response and its metrics via the `HistoryService`.

---

### Case Study 001: "Stop Generation" Does Not Cancel vLLM Request

-   **Artifacts Affected:** `A90. AI Ascent - server.ts (Reference).md`
-   **Cycles Observed:** C70, C75, C99
-   **Symptom:** Clicking the "Stop" button in the response generation UI correctly closes the connection from the DCE extension, but the vLLM server continues to process the request, consuming GPU resources unnecessarily.
-   **Root Cause Analysis (RCA):** The architecture involves the DCE extension making a request to a proxy server, which in turn makes a request to the vLLM server. The `AbortController` in the DCE extension's `llm.service.ts` only aborts the initial request (DCE -> Proxy). The proxy server did not have logic to detect this client-side disconnection and propagate the cancellation to its own downstream request (Proxy -> vLLM). The Express.js response object (`res`) emits a `'close'` event when the client disconnects, which can be used to trigger this cancellation.
-   **Codified Solution & Best Practice:**
    1.  When proxying streaming requests, always propagate client cancellation.
    2.  In the Express.js route handler for the proxy, create a new `AbortController` for the downstream `fetch` request.
    3.  Pass the controller's `signal` to the `fetch` options.
    4.  Register an event listener on the client response object: `res.on('close', () => { controller.abort(); });`. This ensures that if the client hangs up, the server immediately aborts the expensive downstream operation, freeing up resources.