# Artifact A111: DCE - New Regression Case Studies
# Date Created: C99
# Author: AI Model & Curator
# Updated on: C102 (Add Unhandled AbortError in Express Stream)

- **Key/Value for A0:**
- **Description:** Documents new, complex bugs and their codified solutions to prevent future regressions.
- **Tags:** bugs, regression, troubleshooting, development, best practices

## 1. Purpose

This document serves as a living record of persistent or complex bugs. By documenting the root cause analysis (RCA) and the confirmed solution for each issue, we create a "source of truth" to prevent the same mistakes from being reintroduced into the codebase.

## 2. Case Studies

---

### Case Study 005: Unhandled 'AbortError' Crashes Express.js Proxy Server

-   **Artifacts Affected:** `A90. AI Ascent - server.ts (Reference).md`
-   **Cycles Observed:** C102
-   **Symptom:** When a client-side "Stop" action aborts a streaming `fetch` request to an Express.js proxy that is piping a response from a downstream service (like vLLM), the entire Node.js process crashes with a `DOMException [AbortError]: This operation was aborted` and an "Unhandled 'error' event".
-   **Root Cause Analysis (RCA):** The proxy server correctly detects the client disconnection via `res.on('close', ...)` and aborts its own `fetch` request to the downstream service. However, aborting an active `fetch` stream causes the underlying `ReadableStream` to emit an `error` event. In the proxy, the code was piping this stream directly to the client response (`nodeStream.pipe(res)`). When the `nodeStream` emitted the `AbortError`, there was no error handler attached to it (`nodeStream.on('error', ...)`), leading to an unhandled exception that crashed the server.
-   **Codified Solution & Best Practice:**
    1.  Any Node.js `ReadableStream` that is being piped and has the potential to be aborted or encounter an error **must** have an error handler attached.
    2.  The correct implementation is to add a `stream.on('error', (error) => { ... });` handler before calling `stream.pipe()`. This handler should check if the error is an expected `AbortError` and handle it gracefully (e.g., by logging an informational message) while throwing or logging any other unexpected errors. This prevents the process from crashing on a controlled cancellation.

---

### Case Study 004: Proxy Server Aborts vLLM Stream Prematurely

-   **Artifacts Affected:** `A90. AI Ascent - server.ts (Reference).md`
-   **Cycles Observed:** C100, C101
-   **Symptom:** When the DCE extension sends a streaming request to the vLLM via the proxy server, the connection is immediately aborted. The proxy logs show "Client closed the connection. Aborting request to vLLM," even though the client is still waiting for a response.
-   **Root Cause Analysis (RCA):** The logic to handle client-side cancellation was implemented by attaching an event listener to the Express.js `request` object (`req.on('close', ...)`). For a standard HTTP request, the `req` object represents the incoming data from the client. Once the request body is fully received, the `req` stream is finished. However, for a *streaming response*, the long-lived connection is represented by the `response` object (`res`). The `req.on('close')` event was firing prematurely because the initial POST request from the client was completing, which the server misinterpreted as the client disconnecting entirely. The correct event to listen for is `res.on('close')`, which fires only when the client that is *receiving* the streamed response actually closes the connection.
-   **Codified Solution & Best Practice:**
    1.  When implementing cancellation logic for a streaming HTTP response in Express.js, the event listener to detect a client disconnection **must** be attached to the `response` (`res`) object.
    2.  The correct implementation is to use `res.on('close', () => { controller.abort(); });`. This ensures the cancellation is only triggered when the downstream client terminates the connection.

---

### Case Study 003: `AbortController` Lifecycle Bug Causes Unstable Cancellations

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C100
-   **Symptom:** When generating responses, the connection is sometimes aborted immediately and unexpectedly. Furthermore, the manual "Stop Generation" button is unreliable. The root cause is a flaw in how the `AbortController` for the `fetch` request is managed.
-   **Root Cause Analysis (RCA):** The logic for managing streaming `fetch` requests in `generateBatch` and `generateSingle` placed the cleanup code (`generationControllers.delete(cycleId)`) in a `finally` block. For a streaming request, the `await fetch()` promise resolves as soon as the headers are received, allowing the code to proceed while the body streams in asynchronously. Consequently, the `finally` block was executing almost immediately, removing the `AbortController` from the tracking map while the stream was still in progress. This made it impossible to manually cancel the request later and could contribute to unstable connections.
-   **Codified Solution & Best Practice:**
    1.  The lifecycle of a resource tied to a stream (like an `AbortController`) must be managed by the stream's own events, not by a `try/finally` block around the initial `fetch` call.
    2.  The `generationControllers.delete(cycleId)` call must be removed from the `finally` block.
    3.  It must be moved into the terminal event handlers for the stream: `stream.on('end', ...)` and `stream.on('error', ...)`, as well as into the main `catch` block that would handle a failure of the initial `fetch` itself. This ensures the controller is only deregistered when the operation is definitively complete or has failed.

---

### Case Study 002: "Re-generate" Button is Non-Functional

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C75, C99
-   **Symptom:** Clicking the "Re-generate" button on a response tab correctly updates the UI to show a loading/spinning state, but no request is ever sent to the vLLM server, and no new response is generated.
-   **Root Cause Analysis (RCA):** The backend method responsible for this feature, `llm.service.ts::generateSingle`, was an unimplemented stub. The entire frontend and IPC message-passing infrastructure was correctly wired up to call this function, but the function itself contained only placeholder logic to update the UI state to "generating" and did not contain any `fetch` call to actually initiate a new LLM request.
-   **Codified Solution & Best Practice:**
    1.  Ensure all backend service methods intended for user-facing features are fully implemented and not just stubs.
    2.  The `generateSingle` method must be implemented to mirror the core logic of `generateBatch`, but for a single response (`n: 1`).
    3.  It must create and execute a streaming `fetch` request to the configured LLM endpoint.
    4.  It must be able to process the resulting Server-Sent Events (SSE) stream and send granular progress updates back to the client using a dedicated IPC channel (e.g., `UpdateSingleGenerationProgress`) to avoid disrupting the state of other, non-regenerating responses.
    5.  Upon completion, it must persist the final, complete response and its metrics via the `HistoryService`.

---

### Case Study 001: "Stop Generation" Does Not Cancel vLLM Request

-   **Artifacts Affected:** `A90. AI Ascent - server.ts (Reference).md`
-   **Cycles Observed:** C70, C75, C99
-   **Symptom:** Clicking the "Stop" button in the response generation UI correctly closes the connection from the DCE extension, but the vLLM server continues to process the request, consuming GPU resources unnecessarily.
-   **Root Cause Analysis (RCA):** The architecture involves the DCE extension making a request to a proxy server, which in turn makes a request to the vLLM server. The `AbortController` in the DCE extension's `llm.service.ts` only aborts the initial request (DCE -> Proxy). The proxy server did not have logic to detect this client-side disconnection and propagate the cancellation to its own downstream request (Proxy -> vLLM). The Express.js response object (`res`) emits a `'close'` event when the client disconnects, which can be used to trigger this cancellation.
-   **Codified Solution & Best Practice:**
    1.  When proxying streaming requests, always propagate client cancellation.
    2.  In the Express.js route handler for the proxy, create a new `AbortController` for the downstream `fetch` request.
    3.  Pass the controller's `signal` to the `fetch` options.
    4.  Register an event listener on the client response object: `res.on('close', () => { controller.abort(); });`. This ensures that if the client hangs up, the server immediately aborts the expensive downstream operation, freeing up resources.