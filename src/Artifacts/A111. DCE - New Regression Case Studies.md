# Artifact A111: DCE - New Regression Case Studies
# Date Created: C99
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** Documents new, complex bugs and their codified solutions to prevent future regressions.
- **Tags:** bugs, regression, troubleshooting, development, best practices

## 1. Purpose

This document serves as a living record of persistent or complex bugs. By documenting the root cause analysis (RCA) and the confirmed solution for each issue, we create a "source of truth" to prevent the same mistakes from being reintroduced into the codebase.

## 2. Case Studies

---

### Case Study 001: "Stop Generation" Does Not Cancel vLLM Request

-   **Artifacts Affected:** `A90. AI Ascent - server.ts (Reference).md`
-   **Cycles Observed:** C70, C75, C99
-   **Symptom:** Clicking the "Stop" button in the response generation UI correctly closes the connection from the DCE extension, but the vLLM server continues to process the request, consuming GPU resources unnecessarily.
-   **Root Cause Analysis (RCA):** The architecture involves the DCE extension making a request to a proxy server, which in turn makes a request to the vLLM server. The `AbortController` in the DCE extension's `llm.service.ts` only aborts the initial request (DCE -> Proxy). The proxy server did not have logic to detect this client-side disconnection and propagate the cancellation to its own downstream request (Proxy -> vLLM). The Express.js response object (`res`) emits a `'close'` event when the client disconnects, which can be used to trigger this cancellation.
-   **Codified Solution & Best Practice:**
    1.  When proxying streaming requests, always propagate client cancellation.
    2.  In the Express.js route handler for the proxy, create a new `AbortController` for the downstream `fetch` request.
    3.  Pass the controller's `signal` to the `fetch` options.
    4.  Register an event listener on the client response object: `res.on('close', () => { controller.abort(); });`. This ensures that if the client hangs up, the server immediately aborts the expensive downstream operation, freeing up resources.

---

### Case Study 002: "Re-generate" Button is Non-Functional

-   **Artifacts Affected:** `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C75, C99
-   **Symptom:** Clicking the "Re-generate" button on a response tab correctly updates the UI to show a loading/spinning state, but no request is ever sent to the vLLM server, and no new response is generated.
-   **Root Cause Analysis (RCA):** The backend method responsible for this feature, `llm.service.ts::generateSingle`, was an unimplemented stub. The entire frontend and IPC message-passing infrastructure was correctly wired up to call this function, but the function itself contained only placeholder logic to update the UI state to "generating" and did not contain any `fetch` call to actually initiate a new LLM request.
-   **Codified Solution & Best Practice:**
    1.  Ensure all backend service methods intended for user-facing features are fully implemented and not just stubs.
    2.  The `generateSingle` method must be implemented to mirror the core logic of `generateBatch`, but for a single response (`n: 1`).
    3.  It must create and execute a streaming `fetch` request to the configured LLM endpoint.
    4.  It must be able to process the resulting Server-Sent Events (SSE) stream and send granular progress updates back to the client using a dedicated IPC channel (e.g., `UpdateSingleGenerationProgress`) to avoid disrupting the state of other, non-regenerating responses.
    5.  Upon completion, it must persist the final, complete response and its metrics via the `HistoryService`.