# Artifact A95: DCE - LLM Connection Modes Plan
# Date Created: C36
# Author: AI Model & Curator
# Updated on: C37 (Add "Generate Responses" button and streaming metrics)

- **Key/Value for A0:**
- **Description:** Outlines the plan for a multi-modal settings UI and the associated workflow changes, allowing users to switch between manual copy/paste, a pre-configured demo mode, and user-provided API URLs or Keys.
- **Tags:** feature plan, settings, ui, ux, llm, configuration, api, streaming

## 1. Overview & Goal

To maximize the utility and accessibility of the DCE extension, users need a flexible way to connect to different LLM backends. This plan details the implementation of a multi-modal settings UI and the corresponding changes to the main workflow. This will allow users to seamlessly switch between different connection methods, from a simple manual workflow to advanced, automated API integrations.

This plan refines and supersedes `A85. DCE - Model Card Management Plan.md` by focusing on a more user-friendly, mode-based approach.

## 2. User Stories

| ID | User Story | Acceptance Criteria |
|---|---|---|
| P3-CM-01 | **Use Manual Mode** | As a new user, I want the extension to default to a "Free (Manual)" mode, so I can use the core features by copying and pasting without any setup. | - The default setting is "Free Mode". <br> - In this mode, a "Generate prompt.md" button is shown. |
| P3-CM-02 | **Use Demo Mode** | As a demo user, I want to select a "Demo Mode" that connects to a local vLLM endpoint, so I can experience the full automated workflow. | - A "Demo Mode" option is available. <br> - When selected, the "Generate prompt.md" button is replaced with a "Generate responses" button. |
| P3-CM-03 | **Generate Multiple Responses** | As a user in an automated mode, I want to specify how many parallel responses to generate and see them stream into the UI, so I can leverage high-throughput models. | - A number input allows me to select the number of responses. <br> - Clicking "Generate responses" sends a batch request to the configured LLM. <br> - Responses stream into the PCPP tabs and are automatically parsed on completion. |
| P3-CM-04 | **Monitor Generation Speed** | As a user generating responses, I want to see a live "tokens per second" metric, so I have feedback on the generation performance. | - A "Tokens/sec" display appears near the "Generate responses" button during generation. <br> - It updates in real-time as token data streams in. |
| P3-CM-05 | **Persistent Settings** | As a user, I want my selected connection mode to be saved, so I don't have to re-configure it every time I open VS Code. | - The selected connection mode and any associated URL/Key is persisted in the workspace settings. |

## 3. UI/UX Design

### 3.1. Settings Panel
The settings panel will use radio buttons for mode selection with conditional inputs.

```
[ Settings ]
----------------------------------------------------------------------
(â€¢) Free Mode (Manual Copy/Paste)
( ) Demo Mode (Local vLLM via aiascent.game)
( ) API (URL) -> [ http://localhost:8000/v1 ]
----------------------------------------------------------------------
```

### 3.2. Parallel Co-Pilot Panel
-   The "Generate prompt.md" button will be conditionally replaced by a "Generate responses" button.
-   A number input for `responseCount` will appear next to it.
-   A `Tokens/sec: [value]` display will appear during generation.

## 4. Technical Implementation Plan

### 4.1. Settings Persistence (New `SettingsService`)
-   **`src/backend/services/settings.service.ts` (New):** This service will manage getting and setting the active connection mode and any associated URL. It will use `vscode.workspace.getConfiguration` to persist this data.
-   **IPC:** New channels (`RequestSettings`, `SendSettings`, `SaveSettings`) will be created to communicate between the `settings.view.tsx` and the new service.
-   **`settings.view.tsx`:** Will be refactored to fetch its state from and save its state to the new `SettingsService`, fixing the persistence bug.

### 4.2. "Generate Responses" Workflow
-   **`parallel-copilot.view/view.tsx`:**
    -   Will fetch the active connection mode from the `SettingsService`.
    -   Will conditionally render the UI elements based on the mode.
    -   The "Generate responses" button `onClick` handler will:
        1.  Call the `prompt.service.ts` to generate the prompt string *in memory*.
        2.  Send a new `RequestBatchGeneration` IPC message with the prompt string and response count.
-   **`llm.service.ts` (New):**
    -   This new service will handle all outgoing API calls.
    -   Its `generateBatch` method will read the settings, construct the appropriate non-streaming `fetch` request to the vLLM endpoint, and return an array of response strings.
-   **IPC & Frontend Update:**
    -   A `SendBatchGenerationResult` channel will return the array of responses.
    *   The frontend handler will populate the `rawContent` of the response tabs and then trigger `parseAllTabs()`.

### 4.3. Streaming & Metrics (Future Cycle)
-   The `llm.service.ts` will be updated to handle streaming responses.
-   New IPC channels (`StreamResponseChunk`, `StreamResponseEnd`) will be created.
-   The frontend in `view.tsx` will be updated to handle these streaming messages, append content to the tabs in real-time, and calculate the tokens/second metric.