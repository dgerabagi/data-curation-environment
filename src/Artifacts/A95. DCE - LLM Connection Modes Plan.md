# Artifact A95: DCE - LLM Connection Modes Plan
# Date Created: C36
# Author: AI Model & Curator
# Updated on: C42 (Refine "Generate Responses" workflow to create a new cycle first)

- **Key/Value for A0:**
- **Description:** Outlines the plan for a multi-modal settings UI and the associated workflow changes, allowing users to switch between manual copy/paste, a pre-configured demo mode, and user-provided API URLs or Keys.
- **Tags:** feature plan, settings, ui, ux, llm, configuration, api, streaming

## 1. Overview & Goal

To maximize the utility and accessibility of the DCE extension, users need a flexible way to connect to different LLM backends. This plan details the implementation of a multi-modal settings UI and the corresponding changes to the main workflow. This will allow users to seamlessly switch between different connection methods, from a simple manual workflow to advanced, automated API integrations.

This plan refines and supersedes `A85. DCE - Model Card Management Plan.md` by focusing on a more user-friendly, mode-based approach.

## 2. User Stories

| ID | User Story | Acceptance Criteria |
|---|---|---|
| P3-CM-01 | **Use Manual Mode** | As a new user, I want the extension to default to a "Free (Manual)" mode, so I can use the core features by copying and pasting without any setup. | - The default setting is "Free Mode". <br> - In this mode, a "Generate prompt.md" button is shown. |
| P3-CM-02 | **Use Demo Mode** | As a demo user, I want to select a "Demo Mode" that connects to a local vLLM endpoint, so I can experience the full automated workflow. | - A "Demo Mode" option is available. <br> - When selected, the "Generate prompt.md" button is replaced with a "Generate responses" button. |
| P3-CM-03 | **Generate Into New Cycle** | As a user in an automated mode, when I click "Generate responses" on Cycle `N`, I want the extension to automatically create a new Cycle `N+1` and place the generated responses there, so my new results are cleanly separated from the prompt that created them. | - Clicking "Generate responses" initiates a process that creates a new cycle. <br> - The generated responses from the LLM populate the tabs of the new cycle. <br> - The UI automatically navigates to the new cycle upon completion. |
| P3-CM-04 | **Monitor Generation Speed** | As a user generating responses, I want to see a live "tokens per second" metric, so I have feedback on the generation performance. | - A "Tokens/sec" display appears near the "Generate responses" button during generation. <br> - It updates in real-time as token data streams in. |
| P3-CM-05 | **Persistent Settings** | As a user, I want my selected connection mode to be saved, so I don't have to re-configure it every time I open VS Code. | - The selected connection mode and any associated URL/Key is persisted in the workspace settings. |

## 3. UI/UX Design

(No changes from C37)

## 4. Technical Implementation Plan

### 4.1. Settings Persistence
(No changes from C37)

### 4.2. "Generate Responses" Workflow (C42 Update)
The workflow is now designed to be more robust and atomic, with the backend handling the creation of the new cycle.

1.  **Frontend (`view.tsx`):**
    *   The `handleGenerateResponses` `onClick` handler will gather the *current* cycle's data (`PcppCycle` object for Cycle `N`) and send it to the backend via a `RequestBatchGeneration` message.
2.  **Backend (`on-message.ts`):**
    *   The handler for `RequestBatchGeneration` receives the full data for Cycle `N`.
    *   It first calls `prompt.service.ts` to generate the prompt string from Cycle `N`'s data.
    *   It then calls `llm.service.ts` to get the array of response strings from the vLLM.
    *   It then calls a new method in `history.service.ts`, `createNewCycleWithResponses`, passing in the array of responses.
    *   The `history.service.ts` creates the new cycle (`N+1`), populates its response tabs, and saves the entire updated history.
    *   Finally, the backend sends a `SendBatchGenerationComplete` message to the frontend, containing the `newCycleId`.
3.  **Frontend (`view.tsx`):**
    *   A new message handler for `SendBatchGenerationComplete` receives the ID of the new cycle.
    *   It then calls the existing `handleCycleChange` logic to navigate the UI to this new cycle, which now contains all the generated responses.

### 4.3. Streaming & Metrics (Future Cycle)
-   The backend `llm.service.ts` will be updated to handle streaming responses.
-   New IPC channels (`StreamResponseChunk`, `StreamResponseEnd`) will be created.
-   The frontend in `view.tsx` will be updated to handle these streaming messages, append content to the tabs in real-time, and calculate the tokens/second metric.