# Artifact A89: DCE - Phase 3 - Hosted LLM & vLLM Integration Plan
# Date Created: C29
# Author: AI Model & Curator
# Updated on: C39 (Add Troubleshooting section)

- **Key/Value for A0:**
- **Description:** Outlines the architecture and roadmap for integrating the DCE extension with a remote, high-throughput vLLM backend via a secure proxy server.
- **Tags:** feature plan, phase 3, llm, vllm, inference, performance, architecture, proxy

## 1. Vision & Goal

This document outlines the plan for Phase 3 of the Data Curation Environment: transitioning from a manual, copy-paste workflow to a fully integrated, high-performance system powered by a remote LLM. The vision is to combine the parallel response management of the PCPP with the high-throughput parallel generation capabilities of vLLM, allowing a user to generate and review multiple AI responses nearly simultaneously.

The goal is to create an architecture where the DCE extension acts as a client to a centralized, hosted server that proxies requests to a vLLM backend. This enables a seamless, automated workflow from prompt generation to response population.

## 2. Proposed Architecture

The system will consist of three main components:

```
+--------------------------+      +---------------------------+      +----------------------------+
|  DCE Extension (Client)  |----->|  Proxy Server (aiascent)  |----->| vLLM Backend (Inference)   |
| (Sends prompt.md content)|      | (Authenticates & Forwards) |      | (Generates N responses)    |
+--------------------------+      +---------------------------+      +----------------------------+
```

1.  **DCE Extension (Client):** The VS Code extension will be updated to send its generated prompt to a remote server instead of requiring the user to copy-paste. This will be a configurable setting.
2.  **Proxy Server (e.g., `aiascent.game`):** This existing Node.js/Express server will be modified to act as a secure gateway. It will expose a new API endpoint specifically for the DCE. Its role is to receive requests, potentially add authentication or logging, and forward them to the internal vLLM server. This is critical for security, as it keeps the vLLM endpoint from being publicly exposed.
3.  **vLLM Backend:** This is a separate Python process running the vLLM engine with a loaded model. vLLM provides an OpenAI-compatible API endpoint that can accept a prompt and a parameter `n` to generate multiple completions in parallel.

## 3. Security & Encryption

The connection between the DCE Extension and the Proxy Server must be encrypted. The connection between the Proxy Server and the vLLM Backend can be over plain HTTP as it occurs on a trusted, internal network.

-   **Mechanism:** A reverse proxy like Caddy or Nginx should be placed in front of the Node.js Proxy Server. This reverse proxy is responsible for terminating HTTPS traffic, managing TLS certificates (e.g., via Let's Encrypt), and forwarding the decrypted traffic to the Node.js application.
-   **Reference:** For a detailed explanation of this architecture, see **`A93. DCE - vLLM Encryption in Transit Guide.md`**. For a sample configuration, see **`A91. AI Ascent - Caddyfile (Reference).md`**.

## 4. User Stories

| ID | User Story | Acceptance Criteria |
|---|---|---|
| P3-LLM-01 | **Configure Remote Endpoint** | As a curator, I want to configure my DCE extension in the settings panel to point to my hosted proxy server URL, so it knows where to send prompts. | - The settings panel has a field for a "Remote Generation URL". |
| P3-LLM-02 | **Automated Parallel Generation** | As a curator, after generating a `prompt.md`, I want to click a "Generate Responses" button that sends the prompt to my server and automatically populates the PCPP tabs with the 10 parallel responses from vLLM. | - A "Generate Responses" button appears in the PCPP. <br> - Clicking it sends the `prompt.md` content to the configured proxy URL. <br> - The server forwards this to vLLM with `n=10`. <br> - The server returns an array of 10 response strings. <br> - The PCPP automatically populates the content of the first 10 response tabs with the results. |

## 5. Troubleshooting Connectivity

If the extension fails to connect to the proxy server with an `ETIMEDOUT` error, it signifies a network-level problem.

### Step 1: Test the Proxy Server from Your Laptop
The first step is to verify that the proxy server is running and accessible from the machine running VS Code. Open a terminal (e.g., PowerShell, Command Prompt, or a Linux/macOS terminal) and run the following `curl` command.

```bash
curl -X POST -H "Content-Type: application/json" \
-d '{"model": "local-model", "prompt": "This is a test prompt.", "max_tokens": 10}' \
https://aiascent.game/api/dce/proxy -v
```
*   The `-v` (verbose) flag will provide detailed connection information.

### Step 2: Interpret the Results
-   **If `curl` also fails (e.g., times out or "Connection refused"):** The problem is with the `aiascent.game` server or the network path to it. The VS Code extension is not the cause.
    -   Check if the Node.js server is running on the host machine.
    -   Check the Caddy server logs to see if requests are being received.
    -   Check for firewalls on the server or your client machine that might be blocking the connection.
-   **If `curl` succeeds:** This indicates the proxy server is working correctly, and the issue is likely specific to the VS Code extension's environment (e.g., a corporate proxy interfering with Node.js `fetch` calls). The next step would be to add more detailed error logging within the `llm.service.ts` file in the extension.

## 6. Technical Implementation Plan

### Step 1: vLLM Server Setup (Curator Task)
-   Set up a Python environment with vLLM.
-   Run the vLLM server with an OpenAI-compatible endpoint, loading the desired model.
-   Example command: `python -m vllm.entrypoints.openai.api_server --model "my/model"`
-   Ensure this server is running on an internal port (e.g., `8000`).

### Step 2: Proxy Server Modification (`server.ts`)
-   Add a new API endpoint to the Express server: `POST /api/dce/generate`.
-   This endpoint will read the prompt content from the request body.
-   It will construct a new request to the internal vLLM server (e.g., `http://localhost:8000/v1/completions`).
-   The request to vLLM will include `{ "prompt": ..., "n": 10, "max_tokens": 4096 }`.
-   The handler will await the response from vLLM, extract the array of `choices`, and send them back to the DCE client.

### Step 3: DCE Extension Integration (Future Cycle)
-   Add a setting for the proxy URL.
-   Add a "Generate Responses" button to the PCPP UI.
-   Implement the `fetch` call to the proxy server.
-   Implement the logic to take the returned array of responses and populate the `tabs` state in the PCPP.