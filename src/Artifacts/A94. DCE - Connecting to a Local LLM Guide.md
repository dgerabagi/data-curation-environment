# Artifact A94: DCE - Connecting to a Local LLM Guide
# Date Created: C35
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A step-by-step guide on how to configure the DCE extension to use a local LLM with an OpenAI-compatible API.
- **Tags:** guide, setup, llm, vllm, model card, configuration, local

## 1. Overview & Goal

This guide explains how to configure the Data Curation Environment (DCE) extension to communicate with a locally hosted Large Language Model (LLM), such as the one set up via the `A92. DCE - vLLM Setup Guide`.

The goal is to create a "Model Card" in the DCE settings. This card tells the extension where to send its API requests, allowing you to switch from the default manual workflow to a fully integrated, automated one.

## 2. The Model Card Feature

The Model Card system allows you to define and save configurations for different LLMs. The extension can then be set to use any of these saved cards as its "active" model. For a more detailed overview of this feature, see `A85. DCE - Model Card Management Plan.md`.

## 3. Step-by-Step Configuration

### Step 1: Open the Settings Panel
- Open the Command Palette (`Ctrl+Shift+P` or `Cmd+Shift+P`).
- Run the command: **`DCE: Open Settings & Help`**. This will open the settings panel in a new editor tab.

### Step 2: Navigate to the Settings Section
- In the settings panel, find and expand the **"Settings"** section.

### Step 3: Create a New Model Card
- You will see a list of model cards (which will initially only contain the default "AI Studio" card).
- Click the **"Add New Model Card"** button.

### Step 4: Configure the Model Card for vLLM
- A form will appear. Fill it out with the following details for your local vLLM server:
    - **Display Name:** `Local vLLM` (or any name you prefer).
    - **Provider Type:** `OpenAI-Compatible`.
    - **API Endpoint URL:** `http://localhost:8000/v1`
        - **Important:** If you are running the vLLM server on a different machine on your local network, replace `localhost` with that machine's IP address (e.g., `http://192.168.1.100:8000/v1`).
    - **API Key:** Leave this field blank. Your local server does not require an API key.
    - **Context Window Size:** Enter the context window of the model you are serving (e.g., `8192`).
- Click **"Save"**.

### Step 5: Set the New Card as Active
- Your new "Local vLLM" card will now appear in the list.
- Click the **"Select"** or **"Activate"** button next to it.
- A visual indicator should appear, confirming it is now the active model.

## 4. Next Steps

The DCE extension is now configured to send its API requests to your local vLLM server. You can now use the "Generate Responses" button (once implemented in Phase 3) in the Parallel Co-Pilot Panel to automatically populate the response tabs, completing the automated workflow.