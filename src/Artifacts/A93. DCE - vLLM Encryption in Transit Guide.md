# Artifact A93: DCE - vLLM Encryption in Transit Guide
# Date Created: C32
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** Explains the standard architectural pattern of using a reverse proxy to provide HTTPS encryption for the vLLM API endpoint.
- **Tags:** guide, security, encryption, https, proxy, caddy, vllm

## 1. The Challenge: Securing LLM Traffic

When the Data Curation Environment (DCE) extension communicates with a remote vLLM server, the data (which includes source code and prompts) must be encrypted in transit to prevent eavesdropping. The vLLM OpenAI-compatible server runs on plain `http` by default, which is unencrypted. Connecting to an `http` endpoint over the public internet is insecure.

The goal is to provide a secure `https` endpoint for the DCE extension while allowing the vLLM server to run in its default, simple configuration.

## 2. The Solution: The Reverse Proxy Pattern

The standard and most robust solution is to place a **reverse proxy** in front of the vLLM server. The reverse proxy acts as a secure, public-facing gateway.

### 2.1. How It Works

The data flow is as follows:

```
+---------------+      +----------------------+      +----------------------+
| DCE Extension |----->|  Reverse Proxy       |----->|   vLLM Server        |
| (Client)      |      |  (e.g., Caddy/Nginx) |      | (Internal Service)   |
|               |      |                      |      |                      |
| (HTTPS Request)      |  (Handles TLS/SSL)   |      |  (HTTP Request)      |
+---------------+      +----------------------+      +----------------------+
```

1.  **Encrypted Connection:** The DCE extension makes a request to a secure URL, like `https://my-llm-server.com`. This connection is encrypted using HTTPS.
2.  **HTTPS Termination:** The reverse proxy server (e.g., Caddy) receives this encrypted request. Its primary job is to handle the complexity of TLS/SSL certificates. It decrypts the request.
3.  **Forwarding:** After decrypting the request, the proxy forwards it to the internal vLLM server over a trusted local network (e.g., to `http://localhost:8000`). Since this traffic never leaves the secure server environment, it does not need to be re-encrypted.
4.  **Response:** The vLLM server processes the request and sends its `http` response back to the proxy, which then encrypts it and sends it back to the DCE extension over `https`.

### 2.2. Benefits of this Architecture

-   **Security:** All traffic over the public internet is encrypted.
-   **Simplicity:** The vLLM server itself does not need to be configured with complex SSL certificates. Tools like Caddy can automatically provision and renew free Let's Encrypt certificates, making setup very easy.
-   **Flexibility:** The proxy can also handle load balancing, caching, and routing to multiple backend services if needed in the future.

## 3. Implementation Example with Caddy

Caddy is a modern web server that makes this process extremely simple.

-   **Prerequisites:** You need a server with a public IP address and a domain name pointing to it.
-   **Example `Caddyfile`:**
    ```caddy
    # Your domain name
    my-llm-server.com {
        # Caddy will automatically handle HTTPS for this domain
        
        # Log all requests for debugging
        log {
            output file /var/log/caddy/vllm.log
        }

        # Reverse proxy all requests to the vLLM server running on port 8000
        reverse_proxy localhost:8000
    }
    ```
-   **Reference:** For a more detailed example of a production `Caddyfile` used in a similar project, see **`A91. AI Ascent - Caddyfile (Reference).md`**.

This architecture is the industry standard for securing web services and is the recommended approach for deploying the vLLM server for use with the DCE.