# Artifact A85: DCE - Model Card Management Plan
# Date Created: C18
# Author: AI Model & Curator
# Updated on: C29 (Add example for remote vLLM endpoint)

- **Key/Value for A0:**
- **Description:** A plan for an enhanced settings panel where users can create and manage "model cards" to easily switch between different LLM providers and configurations.
- **Tags:** feature plan, settings, ui, ux, llm, configuration, model management

## 1. Overview & Goal

As the DCE project moves towards direct API integration (Phase 3), users will need a flexible way to manage configurations for different Large Language Models (LLMs). The current placeholder for a single local API URL is insufficient.

The goal is to design an enhanced settings panel where users can create and manage "model cards." Each card will represent a specific LLM configuration, containing all the necessary information to interact with that model. This will allow users to easily switch between different models, such as a local model, various cloud providers, or different versions of the same model.

## 2. User Stories

| ID | User Story | Acceptance Criteria |
|---|---|---|
| P3-MC-01 | **Create a New Model Card** | As a user, I want to create a new "model card" in the settings panel, so I can configure the extension to use a new LLM. | - A "New Model Card" button exists in the settings panel. <br> - Clicking it opens a form with fields for Display Name, API Endpoint URL, API Key, Context Window Size, and other relevant parameters. <br> - The default "AI Studio" (Free) mode is presented as a non-editable, built-in card. |
| P3-MC-02 | **Save and Manage Cards** | As a user, I want to save my model cards and see a list of all my configured models, so I can manage my connections. | - Saved model cards are displayed in a list in the settings panel. <br> - I can select a card to view or edit its details. <br> - I can delete a card I no longer need. |
| P3-MC-03 | **Select Active Model** | As a user, I want to select one of my model cards as the "active" model, so the extension knows which LLM to use for its API calls. | - A mechanism (e.g., a radio button or a "Set as Active" button) allows me to designate one model card as the active one. <br> - The extension's backend uses the configuration from the active model card for all subsequent API calls. |

## 3. Proposed UI/UX

The "Settings" section of the settings panel will be redesigned.

1.  **Model Card List:**
    *   A list or grid on the left will display all saved model cards.
    *   The first item will be the default, non-editable "AI Studio (Free/Manual)" card.
    *   A "+" button will allow users to create a new card.
2.  **Configuration Form:**
    *   Selecting a card from the list (or creating a new one) will display its configuration form on the right.
    *   **Fields:**
        *   **Display Name:** (e.g., "My Local Llama3", "Claude Sonnet 3.5")
        *   **Provider Type:** (Dropdown: "Local URL", "OpenAI-Compatible", "Google Gemini", etc.)
        *   **API Endpoint URL:** The URL for the model's API.
        *   **API Key:** A field to securely input the API key.
        *   **Context Window Size (Tokens):** A number input, as some calculations may depend on this.
        *   **Other Parameters:** (Optional) Fields for temperature, top_p, etc.
3.  **Actions:**
    *   "Save" and "Delete" buttons will be present in the form view.
    *   A visual indicator (e.g., a star icon or a "âœ“ Active" badge) will show which card is currently active.

## 4. Example Model Card for Remote vLLM Proxy

This example shows how a user would configure a model card to connect to the `aiascent.game` proxy server, which in turn connects to a vLLM instance.

-   **Display Name:** `AI Ascent vLLM (Remote)`
-   **Provider Type:** `OpenAI-Compatible`
-   **API Endpoint URL:** `https://aiascent.game/api/dce/proxy`
-   **API Key:** (Leave blank, as the proxy server manages authentication)
-   **Context Window Size:** `8192` (or as configured in vLLM)