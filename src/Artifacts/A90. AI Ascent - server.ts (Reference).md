# Artifact A90: AI Ascent - server.ts (Reference)
# Date Created: C29
# Author: AI Model & Curator
# Updated on: C39 (Add enhanced logging to proxy route)

- **Key/Value for A0:**
- **Description:** A reference copy of the `server.ts` file from the `aiascent.game` project, used as a baseline for implementing the DCE LLM proxy.
- **Tags:** reference, source code, backend, nodejs, express

## 1. Overview

This artifact contains the literal source code of the `server.ts` file from the `aiascent.game` project. The key section is the `app.post('/api/dce/proxy', ...)` route, which has been updated with enhanced logging to help diagnose connection issues between the proxy and the backend vLLM server.

## 2. Source Code (with enhanced logging)

```typescript
// ... (imports and other server setup code remains the same) ...
import fetch from 'node-fetch'; 

// ... (rest of the server code) ...

// NEW: DCE LLM Proxy Route (Updated with enhanced logging)
app.post('/api/dce/proxy', async (req, res) => {
    const vllmUrl = process.env.VLLM_URL;
    if (!vllmUrl) {
        logError('[DCE Proxy]', 'VLLM_URL environment variable is not set.');
        return res.status(500).send('vLLM endpoint is not configured on the server.');
    }

    try {
        logInfo('[DCE Proxy]', `Received request. Forwarding to vLLM at ${vllmUrl}`);
        
        const vllmResponse = await fetch(vllmUrl, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({
                ...req.body,
                stream: true,
            }),
        });

        if (!vllmResponse.ok) {
            const errorBody = await vllmResponse.text();
            logError('[DCE Proxy]', `vLLM server returned an error: ${vllmResponse.status} ${vllmResponse.statusText}`, errorBody);
            return res.status(vllmResponse.status).send(errorBody);
        }
        
        logInfo('[DCE Proxy]', 'Successfully connected to vLLM. Piping stream back to client.');
        res.setHeader('Content-Type', 'text/event-stream');
        res.setHeader('Cache-Control', 'no-cache');
        res.setHeader('Connection', 'keep-alive');

        vllmResponse.body.pipe(res);

    } catch (error: any) {
        // Log the specific error to help diagnose timeouts vs. other issues
        logError('[DCE Proxy]', `Error proxying request to vLLM: ${error.message}`, error);
        res.status(500).send(`Internal server error while contacting the vLLM service: ${error.message}`);
    }
});


// ... (rest of server.ts) ...