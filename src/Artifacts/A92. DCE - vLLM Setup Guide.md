# Artifact A92: DCE - vLLM Setup Guide
# Date Created: C30
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A step-by-step guide for setting up the vLLM inference server with an OpenAI-compatible API endpoint for use with the DCE.
- **Tags:** guide, setup, vllm, llm, inference, performance, openai

## 1. Overview & Goal

This guide provides the necessary steps to install `vLLM` and run a large language model with a high-throughput, OpenAI-compatible API server. This will allow the Data Curation Environment (DCE) to connect to a powerful local or remote inference engine, enabling the generation of multiple, parallel responses with very low latency.

## 2. Prerequisites

*   **OS:** Linux (Recommended) or Windows with WSL2.
*   **Python:** Version 3.9 - 3.12.
*   **GPU:** An NVIDIA GPU with CUDA drivers installed. Compute capability 7.0 or higher is recommended (e.g., V100, T4, RTX 20-series or newer).
*   **Package Manager:** `pip` is required. Using a virtual environment manager like `venv` or `conda` is highly recommended to avoid package conflicts.

## 3. Step-by-Step Setup

### Step 1: Create and Activate a Python Virtual Environment

It is crucial to install `vLLM` and its dependencies in an isolated environment.

**Using `venv` (Recommended):**
```bash
# Navigate to your desired directory
python3 -m venv vllm-env
source vllm-env/bin/activate
```

**Using `conda`:**
```bash
conda create -n vllm-env python=3.12 -y
conda activate vllm-env
```

### Step 2: Install vLLM

With your virtual environment activated, install `vLLM` using `pip`. The library will automatically detect your CUDA version and install the appropriate PyTorch binaries.

```bash
pip install vllm
```

**Verification (Optional):**
To ensure the installation was successful, you can run:
```bash
python -m vllm.entrypoints.openai.api_server --help
```
This should display a list of command-line arguments for the server.

### Step 3: Launch the OpenAI-Compatible Server

To serve a model, use the `vllm.entrypoints.openai.api_server` module. You must specify which model you want to load from the Hugging Face Hub using the `--model` argument.

**Example Command:**
This command will download the `unsloth/gpt-oss-20b` model and start a server on `http://localhost:8000`.

```bash
python -m vllm.entrypoints.openai.api_server --model "unsloth/gpt-oss-20b"
```

The first time you run this, it will download the model weights, which may take some time. Subsequent launches will be much faster as it will use the cached model.

### Step 4: Server Customization (Optional Arguments)

You can customize the server's behavior with various command-line arguments.

*   **Change Host/Port:** To make the server accessible on your local network (e.g., from your `aiascent.game` proxy server), use `--host 0.0.0.0`.
    ```bash
    python -m vllm.entrypoints.openai.api_server \
      --model "unsloth/gpt-oss-20b" \
      --host 0.0.0.0 \
      --port 8000
    ```

*   **GPU Utilization (`tensor-parallel-size`):** If you have multiple GPUs, you can use this argument to split the model across them for better performance.
    ```bash
    # Use 2 GPUs
    python -m vllm.entrypoints.openai.api_server \
      --model "unsloth/gpt-oss-20b" \
      --tensor-parallel-size 2
    ```

*   **Trust Remote Code:** Some models require you to trust remote code from their repository to run.
    ```bash
    python -m vllm.entrypoints.openai.api_server \
      --model "unsloth/gpt-oss-20b" \
      --trust-remote-code
    ```

### Step 5: Connect from DCE

Once the server is running, you can configure a "Model Card" in the DCE settings to connect to it.

*   **Endpoint URL:** `http://<SERVER_IP_ADDRESS>:8000/v1/chat/completions` (or `/v1/completions` depending on the model).
*   **API Key:** Can be left blank.

The server is now ready to receive requests from the DCE extension via your proxy.