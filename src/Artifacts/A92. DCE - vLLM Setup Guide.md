# Artifact A92: DCE - vLLM Setup Guide
# Date Created: C30
# Author: AI Model & Curator
# Updated on: C34 (Prioritize WSL setup as the recommended method for Windows)

- **Key/Value for A0:**
- **Description:** A step-by-step guide for setting up the vLLM inference server with an OpenAI-compatible API endpoint for use with the DCE.
- **Tags:** guide, setup, vllm, llm, inference, performance, openai

## 1. Overview & Goal

This guide provides the necessary steps to install `vLLM` and run a large language model with a high-throughput, OpenAI-compatible API server. This allows the Data Curation Environment (DCE) to connect to a powerful local or remote inference engine.

## 2. Recommended Setup for Windows Users: WSL (Windows Subsystem for Linux)

The `vLLM` library and its high-performance dependency `uvloop` are designed for Linux environments. The most stable and officially recommended way to run vLLM on Windows is through WSL.

### Step 1: Open Your WSL Terminal

Open your installed Linux distribution (e.g., Ubuntu) from the Start Menu.

### Step 2: Create and Activate a Python Virtual Environment

It is crucial to install `vLLM` in an isolated environment.

```bash
# Navigate to a suitable directory (e.g., your home directory)
cd ~

# Create a project folder if you don't have one
mkdir -p Projects/vLLM_WSL
cd Projects/vLLM_WSL

# Create the virtual environment
python3 -m venv vllm-env

# Activate the environment
source vllm-env/bin/activate
```
After activation, your terminal prompt should change to show `(vllm-env)`.

### Step 3: Install vLLM and uvloop

With the virtual environment activated, install both `vLLM` and `uvloop` with a single command.

```bash
pip install vllm uvloop
```

### Step 4: Launch the OpenAI-Compatible Server

To serve a model, use the `vllm.entrypoints.openai.api_server` module. You must specify which model you want to load from the Hugging Face Hub using the `--model` argument.

**Example Command:**
This command will download the `unsloth/gpt-oss-20b` model and start a server on `http://localhost:8000`.

```bash
python -m vllm.entrypoints.openai.api_server --model "unsloth/gpt-oss-20b"
```

The first time you run this, it will download the model weights, which may take some time. Subsequent launches will be much faster as it will use the cached model.

## 3. Server Customization & Production Deployment

For details on customizing the server (e.g., changing host/port, using multiple GPUs) and securing it for production use with a reverse proxy, please refer to the main sections below which apply to all environments.

## 4. Legacy / Not Recommended: Native Windows Setup

While not recommended due to library incompatibilities, it is possible to run `vLLM` on native Windows. Be aware that you cannot use `uvloop`.

### Step 1: Create and Activate a Python Virtual Environment

```bash
# In PowerShell or CMD
python -m venv vllm-env
.\vllm-env\Scripts\activate
```

### Step 2: Install vLLM and winloop

Since `uvloop` is not compatible with Windows, you must install `vLLM` by itself. For better performance, you can optionally install `winloop`, a `uvloop` alternative for Windows.

```bash
pip install vllm
pip install winloop  # Optional, for performance
```

### Step 3: Launch the Server

The command is the same as the WSL version.

```bash
python -m vllm.entrypoints.openai.api_server --model "unsloth/gpt-oss-20b"
```

### Troubleshooting Native Windows

-   **Error:** `ModuleNotFoundError: No module named 'uvloop'`
    -   **Cause:** The vLLM API server has a hardcoded dependency on `uvloop`. Even if `winloop` is installed, the server script does not know how to use it automatically.
    -   **Solution:** This is the primary reason the WSL setup is recommended. Modifying the vLLM source code to use `winloop` is not a stable long-term solution.

## 5. Server Customization (All Platforms)

You can customize the server's behavior with various command-line arguments.

*   **Change Host/Port:** To make the server accessible on your local network (e.g., from your `aiascent.game` proxy server), use `--host 0.0.0.0`.
    ```bash
    python -m vllm.entrypoints.openai.api_server \
      --model "unsloth/gpt-oss-20b" \
      --host 0.0.0.0 \
      --port 8000
    ```

*   **GPU Utilization (`tensor-parallel-size`):** If you have multiple GPUs, you can use this argument to split the model across them for better performance.
    ```bash
    # Use 2 GPUs
    python -m vllm.entrypoints.openai.api_server \
      --model "unsloth/gpt-oss-20b" \
      --tensor-parallel-size 2
    ```

*   **Trust Remote Code:** Some models require you to trust remote code from their repository to run.
    ```bash
    python -m vllm.entrypoints.openai.api_server \
      --model "unsloth/gpt-oss-20b" \
      --trust-remote-code
    ```

## 6. Production Deployment & Security

For a production or shared environment, it is **highly recommended** to run the vLLM server behind a reverse proxy (like Caddy or Nginx).

-   **Purpose:** The reverse proxy will handle HTTPS termination (SSL/TLS certificates), providing a secure `https` endpoint for the DCE extension to connect to. The proxy then forwards the traffic to the internal `http://...` vLLM server.
-   **Architecture:** For a detailed explanation of this secure architecture, see **`A89. DCE - Phase 3 - Hosted LLM & vLLM Integration Plan.md`**.
-   **Example:** A sample `Caddyfile` for this purpose can be found in **`A91. AI Ascent - Caddyfile (Reference).md`**.