# Artifact A92: DCE - vLLM Setup Guide
# Date Created: C30
# Author: AI Model & Curator
# Updated on: C45 (Add note about matching model name in proxy)

- **Key/Value for A0:**
- **Description:** A step-by-step guide for setting up the vLLM inference server with an OpenAI-compatible API endpoint for use with the DCE.
- **Tags:** guide, setup, vllm, llm, inference, performance, openai

## 1. Overview & Goal

This guide provides the necessary steps to install `vLLM` and run a large language model with a high-throughput, OpenAI-compatible API server. This will allow the Data Curation Environment (DCE) to connect to a powerful local or remote inference engine.

## 2. Prerequisites

*   **OS:** Linux or Windows with WSL2 (Windows Subsystem for Linux).
*   **Python:** Version 3.9 - 3.12.
*   **GPU:** An NVIDIA GPU with CUDA drivers installed. Compute capability 7.0 or higher is recommended (e.g., V100, T4, RTX 20-series or newer).
*   **Package Manager:** `pip` is required. Using a virtual environment manager like `venv` or `conda` is highly recommended.

## 3. Recommended Method for Windows: Using WSL2


The vLLM server has a dependency on `uvloop`, a library that is not compatible with native Windows. The most reliable and performant way to run vLLM on a Windows machine is within a WSL2 environment.

### Step 1: Install or Verify WSL2
Open PowerShell and check your WSL status.
```powershell
wsl --status
```
If WSL is not installed, run the following command and then restart your machine.
```powershell
wsl --install
```

### Step 2: Set up Python in WSL
Open your WSL terminal (e.g., by typing `wsl` in the Start Menu). Update your package lists and install the necessary Python tools.
```bash
sudo apt update
sudo apt install python3-venv python3-pip -y
```

### Step 3: Create and Activate a Virtual Environment in WSL
It is crucial to install `vLLM` and its dependencies in an isolated environment *inside WSL*.

```bash
# Create a directory for your project
mkdir -p ~/projects/vLLM
cd ~/projects/vLLM

# Create the virtual environment
python3 -m venv vllm-env

# Activate the environment
source vllm-env/bin/activate
```
Your terminal prompt should now be prefixed with `(vllm-env)`.

### Step 4: Install vLLM and uvloop
With the virtual environment activated inside WSL, you can now install `vLLM` and its required dependency `uvloop`.
```bash
pip install vllm uvloop
```

### Step 5: Launch the OpenAI-Compatible Server
This command will download the specified model and start the server.
```bash
python -m vllm.entrypoints.openai.api_server --model "unsloth/gpt-oss-20b"
```
The server will start on `http://localhost:8000` *inside* the WSL environment.

### Step 6: Accessing the Server from Windows
WSL2 automatically forwards network ports to your Windows host machine. This means you can access the vLLM server from your Windows applications (like the DCE extension or your browser) by navigating to **`http://localhost:8000`**.

### Step 7: Verifying the API Endpoint
When you navigate to `http://localhost:8000` in a web browser, you will see a `404 Not Found` error. This is expected and correct. The server is an API endpoint and is not designed to serve a webpage.

To verify that the API is working, run the following `curl` command from your **WSL terminal** (the same one where the server is running). This sends a test prompt to the completions endpoint.

```bash
curl http://localhost:8000/v1/completions \
-H "Content-Type: application/json" \
-d '{
    "model": "unsloth/gpt-oss-20b",
    "prompt": "San Francisco is a",
    "max_tokens": 7,
    "temperature": 0
}'
```

A successful response will be a JSON object that looks something like this:
```json
{"id":"cmpl-a1b2c3d4e5f6","object":"text_completion","created":1677652288,"model":"unsloth/gpt-oss-20b","choices":[{"index":0,"text":" city in Northern California,","logprobs":null,"finish_reason":"length"}],"usage":{"prompt_tokens":5,"total_tokens":12,"completion_tokens":7}}
```
If you receive this JSON response, your vLLM server is running correctly.

### Step 8: Connecting the DCE Extension
Once you have verified the API is running, you are ready to connect the DCE extension to it.

For detailed instructions, please refer to the next guide: **`A94. DCE - Connecting to a Local LLM Guide.md`**.