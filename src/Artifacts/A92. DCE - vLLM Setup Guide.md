# Artifact A92: DCE - vLLM Setup Guide
# Date Created: C30
# Author: AI Model & Curator
# Updated on: C39 (Add Troubleshooting section for connection timeouts)

- **Key/Value for A0:**
- **Description:** A step-by-step guide for setting up the vLLM inference server with an OpenAI-compatible API endpoint for use with the DCE.
- **Tags:** guide, setup, vllm, llm, inference, performance, openai

## 1. Overview & Goal

This guide provides the necessary steps to install `vLLM` and run a large language model with a high-throughput, OpenAI-compatible API server. This will allow the Data Curation Environment (DCE) to connect to a powerful local or remote inference engine.

## 2. Prerequisites
(No changes from C35)

## 3. Recommended Method for Windows: Using WSL2
(No changes from C35)

... (Steps 1-6 remain the same) ...

### Step 7: Verifying the API Endpoint
(No changes from C35)

### Step 8: Connecting the DCE Extension
(No changes from C35)

## 4. Troubleshooting

### Connection Timeouts (`ETIMEDOUT`)
-   **Symptom:** The DCE extension fails to generate a response, and the logs show an error similar to `connect ETIMEDOUT`.
-   **Meaning:** This is a network timeout. It means the extension (the client) tried to connect to your proxy server (e.g., `https://aiascent.game`) but did not receive a response in time.
-   **Diagnosis:** You need to test the connection from your local machine to the proxy server using a tool like `curl`.

#### Testing the Proxy Connection with `curl`

Run the following command from a terminal on your main computer (e.g., PowerShell or Command Prompt, **not** the WSL terminal). This command simulates the request that the DCE extension sends.

```bash
curl -X POST https://aiascent.game/api/dce/proxy -H "Content-Type: application/json" -d "{\"model\": \"local-model\", \"prompt\": \"San Francisco is a\", \"max_tokens\": 7}"
```

**Analyzing the `curl` Result:**
1.  **If the command also times out:** The problem is with the network connection to your proxy server.
    *   Check that your proxy server (e.g., the `aiascent.game` Node.js application) is running.
    *   Check that any firewalls on the server machine are configured to allow incoming traffic on the necessary port (port 443 for HTTPS).
2.  **If the command succeeds (returns JSON):** The public proxy is working correctly. The issue may be specific to the VS Code or extension environment.
3.  **If the command returns an error (e.g., `502 Bad Gateway`):** The proxy server is reachable, but it cannot connect to your internal vLLM server.
    *   Check the logs of your proxy server (the `aiascent.game` server). The enhanced logging will show if it's failing to connect to the `VLLM_URL`.
    *   Ensure your vLLM instance is running correctly inside WSL.
    *   Verify the IP address and port in your `VLLM_URL` environment variable are correct.