# Artifact A92: DCE - vLLM Setup Guide
# Date Created: C30
# Author: AI Model & Curator
# Updated on: C40 (Add PowerShell-specific curl instructions)

- **Key/Value for A0:**
- **Description:** A step-by-step guide for setting up the vLLM inference server with an OpenAI-compatible API endpoint for use with the DCE.
- **Tags:** guide, setup, vllm, llm, inference, performance, openai

## 1. Overview & Goal

This guide provides the necessary steps to install `vLLM` and run a large language model with a high-throughput, OpenAI-compatible API server. This will allow the Data Curation Environment (DCE) to connect to a powerful local or remote inference engine.

## 2. Prerequisites
(No changes from C35)

## 3. Recommended Method for Windows: Using WSL2
(No changes from C35)

... (Steps 1-6 remain the same) ...

### Step 7: Verifying the API Endpoint

(No changes from C35)

#### Note for Windows PowerShell Users

The standard `curl` command provided in the previous step uses syntax that works on Linux, macOS, and with the `curl.exe` application on Windows. However, in a standard Windows PowerShell terminal, `curl` is an alias for a different command called `Invoke-WebRequest`, which uses a different syntax.

If the command above fails with an error about converting a "String" to a "Dictionary", you have two options:

**Option A (Recommended): Use the full `Invoke-WebRequest` command:**
This is the native PowerShell way to make the request. Copy and paste the entire command into your PowerShell terminal.

```powershell
Invoke-WebRequest -Uri "https://aiascent.game/api/dce/proxy" -Method POST -Headers @{"Content-Type"="application/json"} -Body '{"model": "local-model", "prompt": "This is a connectivity test."}' -Verbose
```

**Option B: Use `curl.exe` to bypass the alias:**
If you have `curl.exe` installed (common on modern Windows), you can specify it directly to use the standard syntax.

```powershell
curl.exe -X POST -H "Content-Type: application/json" -d "{\"model\": \"local-model\", \"prompt\": \"This is a connectivity test.\"}" https://aiascent.game/api/dce/llm-proxy -v
```

A successful response from either of these commands will be a JSON object, confirming your API endpoint is working correctly.

### Step 8: Connecting the DCE Extension
(No changes from C35)

## 4. Troubleshooting

### Connection Timeouts (`ETIMEDOUT`)
-   **Symptom:** The DCE extension fails to generate a response, and the logs show an error similar to `connect ETIMEDOUT`.
-   **Meaning:** This is a network timeout. It means the extension (the client) tried to connect to your proxy server (e.g., `https://aiascent.game`) but did not receive a response in time.
-   **Diagnosis:** You need to test the connection from your local machine to the proxy server using a tool like `curl`.

#### Testing the Proxy Connection with `curl`

Run the appropriate command from the "Note for Windows PowerShell Users" section above from a terminal on your main computer (e.g., PowerShell or Command Prompt, **not** the WSL terminal).

**Analyzing the `curl` Result:**
1.  **If the command also times out:** The problem is with the network connection to your proxy server.
    *   Check that your proxy server (e.g., the `aiascent.game` Node.js application) is running.
    *   Check that any firewalls on the server machine are configured to allow incoming traffic on the necessary port (port 443 for HTTPS).
2.  **If the command succeeds (returns JSON):** The public proxy is working correctly. The issue may be specific to the VS Code or extension environment.
3.  **If the command returns an error (e.g., `502 Bad Gateway`):** The proxy server is reachable, but it cannot connect to your internal vLLM server.
    *   Check the logs of your proxy server (the `aiascent.game` server). The enhanced logging will show if it's failing to connect to the `VLLM_URL`.
    *   Ensure your vLLM instance is running correctly inside WSL.
    *   Verify the IP address and port in your `VLLM_URL` environment variable are correct.