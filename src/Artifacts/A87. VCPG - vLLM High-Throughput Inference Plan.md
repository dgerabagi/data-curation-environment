# Artifact A87: VCPG - vLLM High-Throughput Inference Plan

# Date Created: C78
# Author: AI Model
# Updated on: C29 (Add API Proxy Server architecture)

- **Key/Value for A0:**
- **Description:** A research and planning document analyzing the potential of using vLLM for high-throughput, low-latency inference, and detailing the architecture for connecting to it via a secure proxy server.
- **Tags:** guide, research, planning, ai, llm, vllm, inference, performance, proxy

## 1. Vision & Goal

The goal is to investigate and plan the migration of our AI inference backend from the current LM Studio setup to a more performant and scalable solution using **vLLM**. As described by the curator's research, vLLM offers significant performance gains through techniques like continuous batching, which could enable more advanced AI capabilities, such as near-real-time analysis of multiple data streams or providing concurrent, low-latency AI assistance to every user of the DCE extension.

## 2. Analysis of vLLM

Research and community reports highlight several key advantages of vLLM:
-   **High Throughput:** Demonstrations show massive performance increases (e.g., 10,000+ tokens/second on a single high-end GPU).
-   **Continuous Batching:** vLLM's core innovation is its ability to dynamically batch incoming requests. This is highly efficient for serving multiple requests simultaneously, which is key to our goal of generating 10+ parallel responses.
-   **Low Latency:** Sub-100ms time-to-first-token (TTFT) is achievable, which is critical for a responsive user experience.
-   **OpenAI-Compatible Server:** vLLM includes a built-in server that mimics the OpenAI API protocol. This is a critical feature, as it allows our extension and proxy to interact with it using a standard, well-documented interface.

## 3. Proposed Architecture: Secure API Proxy

To securely connect the DCE extension to a powerful vLLM instance, we will use a backend proxy server. This architecture prevents exposing the vLLM server directly to the public internet and gives us a central point of control.

```
+---------------+      +-------------------------+      +----------------------+
| DCE Extension |----->| aiascent.game (Proxy)   |----->|   vLLM Server        |
| (VS Code)     |      | (Node.js/Express)       |      | (Python)             |
+---------------+      +-------------------------+      +----------------------+
```

### 3.1. vLLM Server Setup
-   **Deployment:** The vLLM server will be a dedicated Python application, likely in a Docker container for easy management.
-   **Model:** It can be configured to serve any Hugging Face model compatible with vLLM.
-   **Interface:** It will run the built-in OpenAI-compatible server, listening on a local port (e.g., `8000`).

### 3.2. AI Ascent Proxy Server (`server.ts`)
-   **Role:** The existing `aiascent.game` server will be enhanced to act as a secure proxy.
-   **New Endpoint:** A new API endpoint, `/api/dce/proxy`, will be created.
-   **Logic:**
    1.  This endpoint will receive requests from authenticated DCE extension users.
    2.  It will read the prompt data from the request body.
    3.  It will make a new `fetch` request to the internal vLLM server (e.g., `http://localhost:8000/v1/chat/completions`), forwarding the prompt.
    4.  Crucially, it will **stream** the response from vLLM back to the DCE extension client, providing the low-latency experience we need.

### 3.3. Caddyfile Configuration
-   The existing `Caddyfile` is already configured with a `reverse_proxy` directive that forwards all traffic to the Node.js server. This configuration is sufficient and automatically handles WebSocket upgrades and necessary headers, so no changes are required.

## 4. Implementation Plan (Future Cycle)

1.  **Setup vLLM Server:** Install vLLM and its dependencies, download a model, and run the OpenAI-compatible server.
2.  **Update `server.ts`:** Add the new `/api/dce/proxy` route with the streaming logic.
3.  **Configure DCE:** Update the DCE settings (via a Model Card) to point to the new `https://aiascent.game/api/dce/proxy` endpoint.
4.  **Test:** Send a prompt from the DCE and verify that the response is streamed back from the vLLM server through the proxy.