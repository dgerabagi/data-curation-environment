# Artifact A87: VCPG - vLLM High-Throughput Inference Plan

# Date Created: C78
# Author: AI Model

- **Key/Value for A0:**
- **Description:** A research and planning document analyzing the potential of using vLLM for high-throughput, low-latency inference for JANE, particularly for batched tool calling.
- **Tags:** guide, research, planning, ai, jane, llm, vllm, inference, performance

## 1. Vision & Goal

The goal is to investigate and plan the migration of the VCPG's AI inference backend from the current LM Studio setup to a more performant and scalable solution using **vLLM**. As described by the curator's research, vLLM offers significant performance gains through techniques like continuous batching, which could enable more advanced AI capabilities, such as near-real-time analysis of multiple data streams or providing concurrent, low-latency AI assistance to every user.

## 2. Analysis of Curator's Research

The provided Reddit post highlights several key advantages of vLLM, particularly with the `gpt-oss-20b` model:
-   **High Throughput:** The demonstration of 10,000 tokens/second on a single 4090 GPU is a massive performance increase.
-   **Continuous Batching:** vLLM's core innovation is its ability to dynamically batch incoming requests. This is highly efficient for a multi-user environment like VCPG, where many users might be querying JANE simultaneously. Instead of processing requests one-by-one, vLLM can process them in parallel batches.
-   **Low Latency:** The post mentions sub-100ms time-to-first-token (TTFT), which is critical for a responsive user experience.
-   **Efficient VRAM Usage:** The claim of running 100 agents, each with a large context window, on a single 24GB GPU suggests highly efficient memory management.

The concept of "one vLLM per user" is likely a misunderstanding of the technology. The power of vLLM comes from a *single* instance serving *many* users concurrently through batching, not from running many instances.

## 3. Proposed Architecture

The plan is to replace the LM Studio API endpoint with a self-hosted, OpenAI-compatible server powered by vLLM.

```
+--------------------------+      +---------------------------+      +----------------------+
|   VCPG Frontend (Client) |----->|   VCPG Backend (NestJS)   |----->|   vLLM Server (Python) |
| (Asks JANE a question)   |      | (Acts as a secure proxy)  |      | (Hosts gpt-oss-20b)    |
+--------------------------+      +---------------------------+      +----------------------+
```

### 3.1. vLLM Server Setup

-   **Deployment:** The vLLM server will be deployed as a dedicated Python application, likely within a Docker container on the AI Services Host (192.168.1.85).
-   **Model:** It will be configured to serve the `unsloth/gpt-oss-20b` model.
-   **Interface:** vLLM provides a built-in OpenAI-compatible server. This is critical, as it means our NestJS backend (`ai.service.ts`) will require minimal changes to its `fetch` logicâ€”it will simply point to the new vLLM server URL.

### 3.2. VCPG Backend Integration

-   The `LLM_API_URL` environment variable will be updated to point to the new vLLM server's endpoint.
-   The backend will continue to be responsible for prompt engineering, context injection (RAG), and state management. The backend sends the final prompt to vLLM; vLLM handles the high-speed inference.

## 4. Implementation Plan (Future Cycle)

1.  **Setup vLLM Server:**
    *   Install vLLM and its dependencies on the AI Services Host.
    *   Write a Python script to initialize the `LLM` engine and start the OpenAI-compatible server, loading the `gpt-oss-20b` model.
    *   Containerize this setup using Docker for easy deployment and management.
2.  **Test vLLM Endpoint:**
    *   Directly test the new vLLM server using `curl` or a simple client to ensure it's responding correctly to OpenAI-compatible requests.
3.  **Update VCPG Backend:**
    *   Change the `LLM_API_URL` in the `.env` file to point to the vLLM server.
    *   Test the full loop from the VCPG client, through the NestJS backend, to the vLLM server.
4.  **Performance Benchmarking:**
    *   Once integrated, conduct performance tests to measure the improvement in response time and throughput compared to the LM Studio setup.

This migration promises to significantly enhance JANE's performance and scalability, paving the way for more complex and responsive AI-driven features in the future.