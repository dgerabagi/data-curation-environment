# Artifact A97: DCE - vLLM Response Progress UI Plan
# Date Created: C48
# Author: AI Model & Curator
# Updated on: C58 (Codify correct multi-stream SSE parsing with nested index)

- **Key/Value for A0:**
- **Description:** A plan and textual mockup for a UI to display the progress of incoming vLLM responses, including progress bars and a tokens/second metric. This has been updated to reflect the final, correct streaming architecture.
- **Tags:** feature plan, ui, ux, vllm, progress indicator, metrics, streaming, sse

## 1. Vision & Goal

Generating multiple, large AI responses can take a significant amount of time. To improve the user experience, it's critical to provide clear, real-time feedback that the system is working and to show the progress of the generation. The goal of this feature is to create a dedicated UI that appears during response generation, displaying progress bars and performance metrics for each parallel response being generated by vLLM.

## 2. User Stories

| ID | User Story | Acceptance Criteria |
|---|---|---|
| P3-PROG-01 | **See Generation Progress** | As a user, when I click "Generate responses," I want a UI to immediately appear that shows me the progress of each response being generated, so I know the system is working and not frozen. | - When generation starts, a progress display UI is shown. <br> - It contains a separate progress bar for each of the `N` requested responses. <br> - Each progress bar updates in real-time as tokens are received. |
| P3-PROG-02 | **See Performance Metrics** | As a user, I want to see a live "tokens per second" metric during generation, so I can gauge the performance of the LLM backend. | - The progress UI displays a "Tokens/sec" value. <br> - This value is calculated and updated periodically throughout the generation process. |

## 3. UI Mockup (Textual Description)

The progress UI will be a dedicated component that is conditionally rendered in the PCPP view when `isGenerating` is true.

```
+-----------------------------------------------------------------+
| Generating Responses...                 Tokens/sec: 1234        |
|-----------------------------------------------------------------|
| Total Tokens: 5,120                                             |
|                                                                 |
| Resp 1: [||||||||||||||||||||||||||||||      ] 80% (6553/8192 tk)|
| Resp 2: [||||||||||||||||||||||||||          ] 70% (5734/8192 tk)|
| Resp 3: [|||||||||||||||||||||||||||||||     ] 85% (6963/8192 tk)|
| Resp 4: [||||||||||||||||||||||||            ] 65% (5324/8192 tk)|
+-----------------------------------------------------------------+
```

## 4. Technical Implementation Plan (C58 Revision)

This feature is critically dependent on a correctly implemented end-to-end streaming architecture using Server-Sent Events (SSE).

1.  **vLLM Server:**
    *   Must be started with the OpenAI-compatible API endpoint.
    *   When a request includes `"stream": true` and `n > 1`, it will send an SSE stream. Each `data:` line will contain a JSON object.

2.  **Proxy Server (`A90 server.ts`):**
    *   Must make its request to the vLLM with `"stream": true"`.
    *   It must **not** buffer the response. It must set the client response headers for SSE and pipe the `ReadableStream` from the vLLM response directly to the client.

3.  **Extension Backend (`llm.service.ts`):**
    *   The `generateBatch` method must correctly consume and parse the SSE stream.
    *   **Data Structure of SSE Chunk:** The backend must expect a JSON object on each `data:` line with the following structure:
        ```json
        {
          "choices": [
            {
              "index": 0, // The index of the response stream
              "delta": { "content": "..." },
              "finish_reason": null
            }
          ]
        }
        ```
    *   **Parsing Logic:** The `stream.on('data')` handler must:
        *   Split the incoming buffer by newlines to handle multiple messages at once.
        *   For each line starting with `data: `, parse the JSON.
        *   Correctly access the nested index via **`parsedJson.choices[0].index`**.
        *   Use this index to update the content and token count for the correct response buffer.
        *   Track the `finish_reason` for each index individually. The operation is only complete when all response indices have reported a finish reason.

4.  **IPC Channels (`channels.type.ts`):**
    *   The `ServerToClientChannel.UpdateGenerationProgress` payload will contain the progress data for each response and the overall tokens-per-second.

5.  **Frontend (`view.tsx`, `GenerationProgressDisplay.tsx`):**
    *   The message handler for `UpdateGenerationProgress` will update the state for `generationProgress` and `tps`.
    *   The `GenerationProgressDisplay` component will render the progress bars and token counts based on this state.