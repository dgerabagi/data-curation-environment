# Artifact A97: DCE - vLLM Response Progress UI Plan
# Date Created: C48
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A plan and textual mockup for a UI to display the progress of incoming vLLM responses, including progress bars and a tokens/second metric.
- **Tags:** feature plan, ui, ux, vllm, progress indicator, metrics

## 1. Vision & Goal

When a user clicks "Generate responses," the extension communicates with a potentially remote vLLM server. This process can take anywhere from a few seconds to over a minute, depending on the model and prompt size. Currently, the UI provides no feedback during this time, which can make the application feel unresponsive or broken.

The goal is to provide clear, real-time feedback to the user about the status of their request. This will be achieved with a new, non-modal UI element that appears during generation, showing the progress for each requested response and displaying a live tokens-per-second metric.

## 2. User Stories

| ID | User Story | Acceptance Criteria |
|---|---|---|
| P3-PROG-01 | **See Generation Progress** | As a user, after I click "Generate responses," I want to see a visual indicator that the request is in progress, so I know the application is working. | - A progress display area appears in the UI (e.g., in the header) after "Generate responses" is clicked. <br> - This display shows one progress bar for each response being generated (e.g., 4 bars if 4 responses were requested). |
| P3-PROG-02 | **Monitor Performance** | As a user, I want to see a live tokens-per-second (tok/s) metric, so I can gauge the performance of the LLM backend. | - The progress display includes a "Tokens/sec" counter. <br> - This counter updates in real-time as response data streams in from the backend. |
| P3-PROG-03 | **View Streaming Responses** | As a user, I want to see the text of the responses appear in the tabs as they are being generated, so I can see the results immediately without waiting for all responses to be complete. | - As tokens are generated by the LLM, they are streamed to the appropriate response tab in the newly created cycle. |

## 3. UI Mockup (Textual Description)

The progress display will be a new element that appears within the main header (`pc-header`) of the Parallel Co-Pilot Panel, to the right of the "Generate responses" button.

**State: Before Generation**
```
| [ Project Plan ] [ Generate responses ] [ 4 ] [ Log State ]                                       |
```

**State: During Generation**
```
| [ Project Plan ] [ Generating... (Disabled) ] [ 4 ] [ Abort ]                                     |
|-------------------------------------------------------------------------------------------------|
|  Tokens/sec: [ 1,234 ]                                                                          |
|  Resp 1: [||||||||||||||||||||||||------------------] 50%                                          |
|  Resp 2: [||||||||||||||||||||||||------------------] 50%                                          |
|  Resp 3: [||||||||||||||||||------------------------] 40%                                          |
|  Resp 4: [||||||||||--------------------------------] 30%                                          |
|-------------------------------------------------------------------------------------------------|```

### 3.1. Components Breakdown

-   **Generate Button:** The button's text changes to "Generating..." and it becomes disabled.
-   **Abort Button:** A new "Abort" button appears, allowing the user to cancel the request.
-   **Tokens/sec Display:** A live counter showing the aggregate generation speed.
-   **Progress Bars:**
    -   One progress bar for each requested response.
    -   The progress is based on the `max_tokens` requested vs. tokens received. If `max_tokens` isn't specified, it could be an indeterminate progress bar until the `finish_reason` is received.

## 4. Technical Implementation Plan (High-Level)

This feature depends on **streaming** responses from the vLLM server.

1.  **Backend (`llm.service.ts`):**
    *   The `generateBatch` method must be updated to handle `stream: true`.
    *   It will need to process Server-Sent Events (SSE) from the vLLM response stream.
2.  **IPC Channels:**
    *   New channels will be needed to stream data to the frontend:
        *   `StreamResponseChunk(payload: { responseId: number; chunk: string; })`
        *   `StreamResponseEnd(payload: { responseId: number; finish_reason: string; })`
        *   `UpdateTpsMetric(payload: { tps: number; })`
3.  **Frontend (`view.tsx`):**
    *   New state variables will be needed to manage the progress of each response and the TPS metric.
    *   The UI will be updated to render the progress display conditionally.
    *   New message handlers will listen for the streaming IPC events and update the state in real-time, appending text to the response tabs and updating the progress bars.