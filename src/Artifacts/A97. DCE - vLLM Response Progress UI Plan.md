# Artifact A97: DCE - vLLM Response Progress UI Plan
# Date Created: C48
# Author: AI Model & Curator
# Updated on: C57 (Codify correct multi-stream SSE parsing)

- **Key/Value for A0:**
- **Description:** A plan and textual mockup for a UI to display the progress of incoming vLLM responses, including progress bars and a tokens/second metric. This has been updated to reflect the final, correct streaming architecture.
- **Tags:** feature plan, ui, ux, vllm, progress indicator, metrics, streaming, sse

## 1. Vision & Goal
(No change)

## 2. User Stories
(No change)

## 3. UI Mockup (Textual Description)
(No change)

## 4. Technical Implementation Plan (C57 Revision)

This feature is critically dependent on a correctly implemented end-to-end streaming architecture using Server-Sent Events (SSE).

1.  **vLLM Server:**
    *   Must be started with the OpenAI-compatible API endpoint.
    *   When a request includes `"stream": true` and `n > 1`, it will send an SSE stream containing JSON objects for each of the `n` parallel responses. Each object includes an `index` field to identify which response it belongs to.

2.  **Proxy Server (`A90 server.ts`):**
    *   Must make its request to the vLLM with `"stream": true"`.
    *   Crucially, it must **not** buffer the response. It must set the client response headers for SSE (`Content-Type: text/event-stream`) and pipe the `ReadableStream` from the vLLM response directly to the client response.

3.  **Extension Backend (`llm.service.ts`):**
    *   The `generateBatch` method must be refactored to correctly consume and parse the SSE stream containing multiple interleaved responses.
    *   It will initialize arrays to hold the content and progress for each of the `n` requested responses.
    *   It will listen for `data` events on the stream. For each chunk of data, it will:
        *   Split the chunk by newlines, as a single chunk may contain multiple SSE messages.
        *   Process each line, looking for lines that start with `data: `.
        *   Parse the JSON string following `data: `.
        *   Use the `index` property from the parsed JSON object to identify which response the chunk belongs to.
        *   Append the `content` delta to the correct response's aggregated content string and update its token count.
        *   It will individually track the `finish_reason` for each response index.
    *   **The stream processing will only be considered "ended" after a `finish_reason` has been received for all `n` responses.** This is the critical fix to the premature termination bug.
    *   A throttled function will periodically send an `UpdateGenerationProgress` IPC message to the frontend with the latest metrics and partial content.

4.  **IPC Channels (`channels.type.ts`):**
    *   The `ServerToClientChannel.UpdateGenerationProgress` payload will contain the progress data for each response and the overall tokens-per-second.

5.  **Frontend (`view.tsx`, `GenerationProgressDisplay.tsx`):**
    *   The message handler for `UpdateGenerationProgress` will update the state for `generationProgress`, `tps`, and the `tabs` content with the received partial chunks.
    *   The `GenerationProgressDisplay` component will render a progress bar and a small, read-only preview of the generating text for each response, providing live feedback.