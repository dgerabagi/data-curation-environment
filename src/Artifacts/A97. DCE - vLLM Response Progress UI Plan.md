# Artifact A97: DCE - vLLM Response Progress UI Plan
# Date Created: C48
# Author: AI Model & Curator
# Updated on: C53 (Expand technical plan for streaming)

- **Key/Value for A0:**
- **Description:** A plan and textual mockup for a UI to display the progress of incoming vLLM responses, including progress bars and a tokens/second metric.
- **Tags:** feature plan, ui, ux, vllm, progress indicator, metrics, streaming

## 1. Vision & Goal

When a user clicks "Generate responses," the extension communicates with a potentially remote vLLM server. This process can take anywhere from a few seconds to over a minute. Currently, the UI provides no feedback during this time, which can make the application feel unresponsive or broken.

The goal is to provide clear, real-time feedback to the user about the status of their request. This will be achieved with a new UI element that appears during generation, showing the progress for each requested response and displaying a live tokens-per-second metric.

## 2. User Stories

| ID | User Story | Acceptance Criteria |
|---|---|---|
| P3-PROG-01 | **See Generation Progress** | As a user, after I click "Generate responses," I want to see a visual indicator that the request is in progress, so I know the application is working. | - A progress display area appears in the UI after "Generate responses" is clicked. <br> - This display shows one progress bar for each response being generated. |
| P3-PROG-02 | **Monitor Performance** | As a user, I want to see a live tokens-per-second (tok/s) metric, so I can gauge the performance of the LLM backend. | - The progress display includes a "Tokens/sec" counter that updates in real-time. |
| P3-PROG-03 | **View Streaming Responses** | As a user, I want to see the text of the responses appear in the tabs as they are being generated, so I can see the results immediately. | - As tokens are generated by the LLM, they are streamed to the appropriate response tab in the newly created cycle. |
| P3-PROG-04 | **Accurate Progress Metrics** | As a user, I want the progress bar to accurately reflect the generation progress based on the maximum tokens requested, and to see the current token count for each response. | - The progress bar's value is calculated as `currentTokens / maxTokens`. <br> - A text display shows the current and total tokens for each response (e.g., "4096 / 8192 tokens"). <br> - A grand total of all generated tokens is also displayed. |

## 3. UI Mockup (Textual Description)

The progress display will appear in a split-view layout during the onboarding generation. The left side will contain the user's project scope, and the right side will contain the progress UI.

```
|-------------------------------------------------------------------------------------------------|
| [Project Scope Text Area...]                                  | [Generation Progress Display]   |
|                                                               |  Tokens/sec: [ 1,234 ]          |
|                                                               |  Total Tokens: [ 12,345 ]       |
|                                                               |  -----------------------------  |
|                                                               |  Resp 1: [|||||||||||--] 80%    |
|                                                               |   (6553 / 8192 tokens)          |
|                                                               |  Resp 2: [|||||||||||--] 80%    |
|                                                               |   (6553 / 8192 tokens)          |
|                                                               |  Resp 3: [|||||||----] 60%      |
|                                                               |   (4915 / 8192 tokens)          |
|                                                               |  Resp 4: [|||||------] 40%      |
|                                                               |   (3276 / 8192 tokens)          |
|-------------------------------------------------------------------------------------------------|
```

## 4. Technical Implementation Plan (Expanded for Streaming)

This feature is critically dependent on **streaming** responses from the vLLM server.

1.  **Backend (`llm.service.ts`):**
    *   The `generateBatch` method must be refactored to handle a streaming connection.
    *   The `fetch` request to the vLLM proxy must include `stream: true` in its body.
    *   The response from `fetch` will be a stream of Server-Sent Events (SSE). The service will need to read this stream chunk by chunk.
    *   Each chunk will contain a JSON object with a token delta. The service will parse these chunks, aggregate the text for each response, and calculate performance metrics (tokens per second).

2.  **IPC Channels:**
    *   New, dedicated IPC channels are required to push real-time updates from the backend to the frontend.
        *   `ServerToClientChannel.UpdateGenerationProgress`: This message will be sent frequently (e.g., every 200ms).
        *   **Payload:** An array of progress objects, e.g., `[{ responseId: 1, currentTokens: 1024, totalTokens: 8192, chunk: "new text..." }, ...]`. It will also include the overall `tokensPerSecond` metric.
        *   `ServerToClientChannel.SendBatchGenerationComplete`: This existing channel will be used to signal that all response streams have finished.

3.  **Frontend (`view.tsx`):**
    *   **State Management:** New state variables will be required to hold the progress data for each response:
        ```typescript
        interface GenerationProgress {
          responseId: number;
          currentTokens: number;
          totalTokens: number;
          content: string;
        }
        const [progress, setProgress] = useState<GenerationProgress[]>([]);
        const [tps, setTps] = useState(0);
        ```
    *   **Message Handler:** A new message handler will listen for the `UpdateGenerationProgress` IPC message.
        *   When a message is received, it will update the `progress` state with the new token counts and append the `chunk` to the appropriate response tab's content.
        *   It will also update the `tps` state.
    *   The `GenerationProgressDisplay` component will be driven by this state, causing the progress bars and token counters to update in real-time.

4.  **Configuration:**
    *   A constant, `MAX_TOKENS_PER_RESPONSE`, will be defined (e.g., `8192`). This will be used by both the backend when making the request and the frontend for calculating progress percentages. In the future, this could be made a configurable setting.