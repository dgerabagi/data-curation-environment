# Artifact A97: DCE - vLLM Response Progress UI Plan
# Date Created: C48
# Author: AI Model & Curator
# Updated on: C56 (Codify correct SSE parsing architecture for multiple streams)

- **Key/Value for A0:**
- **Description:** A plan and textual mockup for a UI to display the progress of incoming vLLM responses, including progress bars and a tokens/second metric. This has been updated to reflect the final, correct streaming architecture.
- **Tags:** feature plan, ui, ux, vllm, progress indicator, metrics, streaming, sse

## 1. Vision & Goal
(No change)

## 2. User Stories
(No change)

## 3. UI Mockup (Textual Description)
(No change)

## 4. Technical Implementation Plan (C56 Revision)

This feature is critically dependent on a correctly implemented end-to-end streaming architecture using Server-Sent Events (SSE).

1.  **vLLM Server:**
    *   Must be started with the OpenAI-compatible API endpoint.
    *   When a request includes `"stream": true`, it will send an SSE stream containing interleaved JSON objects for each of the `n` parallel responses. Each object includes an `index` field to identify which response it belongs to.

2.  **Proxy Server (`A90 server.ts`):**
    *   Must make its request to the vLLM with `"stream": true"`.
    *   Crucially, it must **not** buffer the response. It must set the client response headers for SSE (`Content-Type: text/event-stream`) and pipe the `ReadableStream` from the vLLM response directly to the client response.

3.  **Extension Backend (`llm.service.ts`):**
    *   The `generateBatch` method must be refactored to correctly consume and parse the SSE stream containing multiple interleaved responses.
    *   It will initialize arrays to hold the content and progress for each of the `n` requested responses.
    *   It will listen for `data` events on the stream. For each chunk of data, it will:
        *   Split the chunk by newlines, as a single chunk may contain multiple SSE messages.
        *   Process each line, looking for lines that start with `data: `.
        *   Parse the JSON string following `data: `.
        *   Use the `index` property from the parsed JSON object to identify which response the chunk belongs to.
        *   Append the `content` delta to the correct response's aggregated content string and update its token count.
    *   A throttled or debounced function will periodically send an `UpdateGenerationProgress` IPC message to the frontend. This message will contain the complete, up-to-date progress array and the partial content chunks for each response.
    *   The stream processing will only be considered "ended" after all `n` responses have been fully received.

4.  **IPC Channels (`channels.type.ts`):**
    *   The `ServerToClientChannel.UpdateGenerationProgress` payload is updated to include the partial content: `{ progress: GenerationProgress[], tps: number, chunks: { [responseId: number]: string } }`.

5.  **Frontend (`view.tsx`, `GenerationProgressDisplay.tsx`):**
    *   The message handler for `UpdateGenerationProgress` will update the state for `generationProgress`, `tps`, and now also the `tabs` content with the received partial chunks.
    *   The `GenerationProgressDisplay` component will be updated to receive the `tabs` data and render a small, read-only preview of the generating text for each response, providing live feedback.