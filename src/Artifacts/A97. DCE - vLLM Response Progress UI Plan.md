# Artifact A97: DCE - vLLM Response Progress UI Plan
# Date Created: C48
# Author: AI Model & Curator
# Updated on: C61 (Update UI mockup and technical plan for unused token count)

- **Key/Value for A0:**
- **Description:** A plan and textual mockup for a UI to display the progress of incoming vLLM responses, including color-coded progress bars, status indicators, and a tokens/second metric.
- **Tags:** feature plan, ui, ux, vllm, progress indicator, metrics, streaming, sse

## 1. Vision & Goal

Generating multiple, large AI responses can take a significant amount of time. To improve the user experience, it's critical to provide clear, real-time feedback that the system is working and to show the progress of the generation. The goal of this feature is to create a dedicated UI that appears during response generation, displaying progress bars, status indicators, and performance metrics for each parallel response being generated by vLLM.

## 2. User Stories

| ID | User Story | Acceptance Criteria |
|---|---|---|
| P3-PROG-01 | **See Generation Progress** | As a user, when I click "Generate responses," I want a UI to immediately appear that shows me the progress of each response being generated, so I know the system is working and not frozen. | - When generation starts, a progress display UI is shown. <br> - It contains a separate progress bar for each of the `N` requested responses. <br> - Each progress bar updates in real-time as tokens are received. |
| P3-PROG-02 | **See Performance Metrics** | As a user, I want to see a live "tokens per second" metric during generation, so I can gauge the performance of the LLM backend. | - The progress UI displays a "Tokens/sec" value. <br> - This value is calculated and updated periodically throughout the generation process. |
| P3-PROG-03 | **Understand Progress Bar**| As a user, I want the progress bar to be color-coded so I can understand the allocation of tokens for the prompt versus the generated response. | - The progress bar is a stacked bar with multiple colors. <br> - One color represents the "thinking" (prompt) tokens. <br> - A second color represents the currently generated response tokens. <br> - A third color (gray) represents the remaining, unused tokens up to the model's maximum. |
| P3-PROG-04 | **See Response Status** | As a user, I want to see the status of each individual response (e.g., "Thinking...", "Generating...", "Complete"), so I know what the system is doing. | - A text indicator next to each progress bar shows its current status. <br> - The indicator is animated during the "Thinking" and "Generating" phases. <br> - When a response is complete, the "unused" portion of its progress bar changes color to signify completion. |
| P3-PROG-05 | **See Unused Tokens** | As a user, once a response is complete, I want to see how many tokens were left unused, so I can understand how much headroom the model had. | - After a response's status changes to "Complete", a text element appears showing the count of unused tokens. |

## 3. UI Mockup (Textual Description)

The progress UI will be a dedicated component that is conditionally rendered in the PCPP view when `isGenerating` is true.

```
+----------------------------------------------------------------------+
| Generating Responses...                  Tokens/sec: 1234            |
|----------------------------------------------------------------------|
| Total Tokens: 18,120                                                 |
|                                                                      |
| Resp 1: [blue|green|gray]  80% | Status: Generating...               |
|         (1k+5.5k/8.1k tk)                                            |
| Resp 2: [blue|green|gray]  70% | Status: Generating...               |
|         (1k+4.7k/8.1k tk)                                            |
| Resp 3: [blue|gray      ]  12% | Status: Thinking...                 |
|         (1k+0k/8.1k tk)                                              |
| Resp 4: [blue|green|done] 100% | Status: Complete âœ“                  |
|         (1k+7.1k/8.1k tk)      | Unused: 1,024 tk                    |
+----------------------------------------------------------------------+```
*   **[blue|green|gray/done]**: Represents the stacked progress bar.
    *   `blue`: "Thinking" tokens (pre-JSON stream).
    *   `green`: Generated response tokens.
    *   `gray`: Unused tokens up to the max.
    *   `done`: A darker gray or other color indicating the response is finished and no more tokens will be generated.

## 4. Technical Implementation Plan (C61 Revision)

This feature is critically dependent on a correctly implemented end-to-end streaming architecture using Server-Sent Events (SSE).

1.  **IPC (`channels.type.ts`):** The `GenerationProgress` interface will be updated to include `thinkingTokens: number`.

2.  **Backend (`llm.service.ts`):**
    *   The `generateBatch` method will manage the lifecycle for each response.
    *   The `stream.on('data')` handler will be enhanced to differentiate between chunks containing `delta.reasoning_content` (thinking tokens) and `delta.content` (response tokens).
    *   The token count from `reasoning_content` will be added to the `thinkingTokens` field for each response.
    *   The token count from `content` will be added to the `currentTokens` field.
    *   The `status` field for each response will be updated: `pending` -> `thinking` -> `generating` -> `complete`.
    *   The `UpdateGenerationProgress` IPC message will send this complete state to the frontend.

3.  **Frontend (`GenerationProgressDisplay.tsx` & `view.scss`):**
    *   **Component (`GenerationProgressDisplay.tsx`):** The component will be refactored to render the new three-part stacked progress bar based on `thinkingTokens`, `currentTokens`, and `totalTokens`.
    *   **Styling (`view.scss`):** The CSS will be updated with colors for each segment (`.thinking`, `.generated`, `.unused`).
    *   **Unused Tokens (`GenerationProgressDisplay.tsx`):** A new `<span>` or `<div>` will be added. It will be conditionally rendered with `display: block` only when the response `status` is `'complete'`. Its content will be the calculated `totalTokens - thinkingTokens - currentTokens`.

4.  **Parsing Logic (SSE Stream):** The backend stream consumer must correctly parse the SSE stream, expecting a JSON object with a nested `choices[0].index` field to correctly route token deltas to the right progress buffer.