# Artifact A97: DCE - vLLM Response Progress UI Plan
# Date Created: C48
# Author: AI Model & Curator
# Updated on: C60 (Update UI mockup and technical plan for color-coding and status)

- **Key/Value for A0:**
- **Description:** A plan and textual mockup for a UI to display the progress of incoming vLLM responses, including color-coded progress bars, status indicators, and a tokens/second metric.
- **Tags:** feature plan, ui, ux, vllm, progress indicator, metrics, streaming, sse

## 1. Vision & Goal

Generating multiple, large AI responses can take a significant amount of time. To improve the user experience, it's critical to provide clear, real-time feedback that the system is working and to show the progress of the generation. The goal of this feature is to create a dedicated UI that appears during response generation, displaying progress bars, status indicators, and performance metrics for each parallel response being generated by vLLM.

## 2. User Stories

| ID | User Story | Acceptance Criteria |
|---|---|---|
| P3-PROG-01 | **See Generation Progress** | As a user, when I click "Generate responses," I want a UI to immediately appear that shows me the progress of each response being generated, so I know the system is working and not frozen. | - When generation starts, a progress display UI is shown. <br> - It contains a separate progress bar for each of the `N` requested responses. <br> - Each progress bar updates in real-time as tokens are received. |
| P3-PROG-02 | **See Performance Metrics** | As a user, I want to see a live "tokens per second" metric during generation, so I can gauge the performance of the LLM backend. | - The progress UI displays a "Tokens/sec" value. <br> - This value is calculated and updated periodically throughout the generation process. |
| P3-PROG-03 | **Understand Progress Bar**| As a user, I want the progress bar to be color-coded so I can understand the allocation of tokens for the prompt versus the generated response. | - The progress bar is a stacked bar with multiple colors. <br> - One color represents the "thinking" (prompt) tokens. <br> - A second color represents the currently generated response tokens. <br> - A third color (gray) represents the remaining, unused tokens up to the model's maximum. |
| P3-PROG-04 | **See Response Status** | As a user, I want to see the status of each individual response (e.g., "Thinking...", "Generating...", "Complete"), so I know what the system is doing. | - A text indicator next to each progress bar shows its current status. <br> - The indicator is animated during the "Thinking" and "Generating" phases. <br> - When a response is complete, the "unused" portion of its progress bar changes color to signify completion. |

## 3. UI Mockup (Textual Description)

The progress UI will be a dedicated component that is conditionally rendered in the PCPP view when `isGenerating` is true.

```
+----------------------------------------------------------------------+
| Generating Responses...                  Tokens/sec: 1234            |
|----------------------------------------------------------------------|
| Total Tokens: 18,120                                                 |
|                                                                      |
| Resp 1: [blue|green|gray]  80% (1k+5.5k/8.1k) | Status: Generating...|
| Resp 2: [blue|green|gray]  70% (1k+4.7k/8.1k) | Status: Generating...|
| Resp 3: [blue|gray      ]  12% (1k+0k/8.1k)   | Status: Thinking...  |
| Resp 4: [blue|green|done] 100% (1k+7.1k/8.1k) | Status: Complete âœ“   |
+----------------------------------------------------------------------+```
*   **[blue|green|gray/done]**: Represents the stacked progress bar.
    *   `blue`: "Thinking" tokens (pre-JSON stream).
    *   `green`: Generated response tokens.
    *   `gray`: Unused tokens up to the max.
    *   `done`: A darker gray or other color indicating the response is finished and no more tokens will be generated.

## 4. Technical Implementation Plan (C60 Revision)

This feature is critically dependent on a correctly implemented end-to-end streaming architecture using Server-Sent Events (SSE).

1.  **IPC (`channels.type.ts`):** The `GenerationProgress` interface will be updated to include `promptTokens: number` and `status: 'pending' | 'thinking' | 'generating' | 'complete' | 'error'`.

2.  **Backend (`llm.service.ts`):**
    *   The `generateBatch` method will manage the lifecycle for each response.
    *   The `stream.on('data')` handler will be enhanced to differentiate between initial unstructured "thinking" text and the structured `data: {...}` SSE messages.
    *   The length of the initial "thinking" text will be summed and stored in the `promptTokens` field for each response.
    *   The `status` field for each response will be updated: `pending` -> `thinking` (on first chunk) -> `generating` (on first `delta.content`) -> `complete` (on `finish_reason`).
    *   The `UpdateGenerationProgress` IPC message will send this complete state to the frontend.

3.  **Frontend (`GenerationProgressDisplay.tsx` & `view.scss`):**
    *   **Layout (`view.scss`):** The CSS for `.generation-progress-display` will be fixed to prevent horizontal overflow, likely by ensuring it respects flexbox/grid container constraints and does not have a hardcoded minimum width.
    *   **Component (`GenerationProgressDisplay.tsx`):** The component will be refactored to render the new three-part stacked progress bar based on `promptTokens`, `currentTokens`, and `totalTokens`.
    *   **Styling (`view.scss`):** The CSS will be updated with the correct colors for each segment (`.thinking`, `.generated`, `.unused`). A new class, `.completed`, will be added to change the color of the `.unused` segment when the response `status` is `'complete'`.
    *   **Status (`GenerationProgressDisplay.tsx`):** The component will render the `status` string and an appropriate icon (e.g., a spinner for thinking/generating, a checkmark for complete).

4.  **Parsing Logic (SSE Stream):** The backend stream consumer must correctly parse the SSE stream, expecting a JSON object with a nested `choices[0].index` field to correctly route token deltas to the right progress buffer.