# Artifact A97: DCE - vLLM Response Progress UI Plan
# Date Created: C48
# Author: AI Model & Curator
# Updated on: C59 (Update UI mockup and technical plan for color-coding and status)

- **Key/Value for A0:**
- **Description:** A plan and textual mockup for a UI to display the progress of incoming vLLM responses, including color-coded progress bars, status indicators, and a tokens/second metric.
- **Tags:** feature plan, ui, ux, vllm, progress indicator, metrics, streaming, sse

## 1. Vision & Goal

Generating multiple, large AI responses can take a significant amount of time. To improve the user experience, it's critical to provide clear, real-time feedback that the system is working and to show the progress of the generation. The goal of this feature is to create a dedicated UI that appears during response generation, displaying progress bars, status indicators, and performance metrics for each parallel response being generated by vLLM.

## 2. User Stories

| ID | User Story | Acceptance Criteria |
|---|---|---|
| P3-PROG-01 | **See Generation Progress** | As a user, when I click "Generate responses," I want a UI to immediately appear that shows me the progress of each response being generated, so I know the system is working and not frozen. | - When generation starts, a progress display UI is shown. <br> - It contains a separate progress bar for each of the `N` requested responses. <br> - Each progress bar updates in real-time as tokens are received. |
| P3-PROG-02 | **See Performance Metrics** | As a user, I want to see a live "tokens per second" metric during generation, so I can gauge the performance of the LLM backend. | - The progress UI displays a "Tokens/sec" value. <br> - This value is calculated and updated periodically throughout the generation process. |
| P3-PROG-03 | **Understand Progress Bar**| As a user, I want the progress bar to be color-coded so I can understand the allocation of tokens for the prompt versus the generated response. | - The progress bar is a stacked bar with three colors. <br> - One color represents the "thinking" (prompt) tokens. <br> - A second color represents the currently generated response tokens. <br> - A third color represents the remaining, unused tokens up to the model's maximum. |
| P3-PROG-04 | **See Response Status** | As a user, I want to see the status of each individual response (e.g., "Thinking...", "Generating...", "Complete"), so I know what the system is doing. | - A text indicator next to each progress bar shows its current status. <br> - The indicator is animated during the "Thinking" and "Generating" phases. |

## 3. UI Mockup (Textual Description)

The progress UI will be a dedicated component that is conditionally rendered in the PCPP view when `isGenerating` is true.

```
+-----------------------------------------------------------------+
| Generating Responses...                 Tokens/sec: 1234        |
|-----------------------------------------------------------------|
| Total Tokens: 18,120                                            |
|                                                                 |
| Resp 1: [blue|green|gray] 80% (1k+5.5k/8.1k) | Status: Generating... |
| Resp 2: [blue|green|gray] 70% (1k+4.7k/8.1k) | Status: Generating... |
| Resp 3: [blue|gray    ] 12% (1k+0k/8.1k)     | Status: Thinking...   |
| Resp 4: [blue|green|gray] 100% (1k+7.1k/8.1k)| Status: Complete âœ“    |
+-----------------------------------------------------------------+
```
*   **[blue|green|gray]**: Represents the stacked progress bar.
    *   `blue`: Prompt/Thinking tokens.
    *   `green`: Generated response tokens.
    *   `gray`: Unused tokens up to the max.

## 4. Technical Implementation Plan (C58 Revision)

This feature is critically dependent on a correctly implemented end-to-end streaming architecture using Server-Sent Events (SSE).

1.  **Backend (`prompt.service.ts`):** A new utility method will be added to calculate the token count of a given prompt string.
2.  **Backend (`llm.service.ts`):**
    *   Before sending the batch request, it will call the new prompt service utility to get the `promptTokens` count.
    *   The `UpdateGenerationProgress` IPC payload will be enhanced to include `promptTokens` and a new `status` string for each response.
    *   The service will manage the lifecycle of the `status` field: initializing it to `'thinking'`, changing it to `'generating'` when the first token delta for that response index is received, and changing it to `'complete'` when a `finish_reason` is received.
3.  **Frontend (`GenerationProgressDisplay.tsx` & `view.scss`):**
    *   The component will be refactored to receive the enhanced progress data.
    *   It will render the progress bar as a container `div` with three child `div`s inside.
    *   The `width` of each child `div` will be set as a percentage of the total `max_tokens` to create the stacked bar effect.
    *   A new element will render the `status` string, with a CSS class applied to trigger an animation for the "Thinking" and "Generating" states.
    *   A tooltip will be added to the TPS display.
4.  **Parsing Logic (SSE Stream):** The backend stream consumer must correctly parse the SSE stream, expecting a JSON object with a nested `choices[0].index` field to correctly route token deltas to the right progress buffer.