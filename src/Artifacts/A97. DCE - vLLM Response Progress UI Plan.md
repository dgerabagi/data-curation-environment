# Artifact A97: DCE - vLLM Response Progress UI Plan
# Date Created: C48
# Author: AI Model & Curator
# Updated on: C54 (Codify final streaming architecture)

- **Key/Value for A0:**
- **Description:** A plan and textual mockup for a UI to display the progress of incoming vLLM responses, including progress bars and a tokens/second metric. This has been updated to reflect the final, correct streaming architecture.
- **Tags:** feature plan, ui, ux, vllm, progress indicator, metrics, streaming, sse

## 1. Vision & Goal
(No change)

## 2. User Stories
(No change)

## 3. UI Mockup (Textual Description)
(No change)

## 4. Technical Implementation Plan (C54 Update)

This feature is critically dependent on an end-to-end streaming architecture using Server-Sent Events (SSE).

1.  **vLLM Server:**
    *   The server must be started with the OpenAI-compatible API endpoint.
    *   It will automatically handle streaming when a request includes `"stream": true`.

2.  **Proxy Server (`A90 server.ts`):**
    *   The proxy must make its request to the vLLM `/v1/chat/completions` endpoint with `"stream": true"` in the body.
    *   It must **not** buffer the response. It will set the client response headers for SSE (`Content-Type: text/event-stream`) and pipe the response body stream from vLLM directly to the client.

3.  **Extension Backend (`llm.service.ts`):**
    *   The `generateBatch` method must be refactored to handle a streaming connection.
    *   It will use a library capable of consuming SSE streams (e.g., `node-fetch` with stream handling or a dedicated SSE client library).
    *   It will listen for `data` events on the stream. Each event will contain a JSON string (e.g., `data: {...}`).
    *   The service will parse these JSON chunks to extract the token deltas for each parallel response.
    *   It will aggregate token counts, calculate performance metrics (tokens per second), and periodically (e.g., every 200ms) send an `UpdateGenerationProgress` IPC message to the frontend.

4.  **IPC Channels:**
    *   A new channel, `ServerToClientChannel.UpdateGenerationProgress`, is required.
    *   **Payload:** `{ progress: GenerationProgress[], tps: number }`, where `GenerationProgress` is `{ responseId: number, currentTokens: number, totalTokens: number }`.

5.  **Frontend (`view.tsx`):**
    *   **State Management:** New state variables are required: `generationProgress: GenerationProgress[]` and `tps: number`.
    *   **Message Handler:** A new message handler will listen for the `UpdateGenerationProgress` IPC message.
        *   When a message is received, it will update the `generationProgress` and `tps` state.
    *   The `GenerationProgressDisplay` component will be driven by this state, causing the UI to update in real-time.
    *   The `SendBatchGenerationComplete` message will signal that all response streams have finished, at which point the UI will navigate to the new cycle.