# Artifact A11: DCE - Regression Case Studies
# Date Created: C16
# Author: AI Model & Curator
# Updated on: C71 (Add Stale Prompt Context case)

## 1. Purpose

This document serves as a living record of persistent or complex bugs that have recurred during development. By documenting the root cause analysis (RCA) and the confirmed solution for each issue, we create a "source of truth" that can be referenced to prevent the same mistakes from being reintroduced into the codebase.

**This artifact is the primary log for new and recent case studies.** Older, resolved issues are archived in `A11.1 DCE - New Regression Case Studies.md` to keep this document concise and focused on currently relevant issues.

## 2. Case Studies

---

### Case Study 040: Stale Prompt Context in Automated Generation Workflow

-   **Artifacts Affected:** `src/backend/services/prompt.service.ts`, `src/client/views/parallel-copilot.view/on-message.ts`, `src/client/views/parallel-copilot.view/view.tsx`
-   **Cycles Observed:** C67, C71
-   **Symptom:** When using the "Generate responses" button for a new cycle (e.g., Cycle 2), the responses received from the LLM are based on the context of a much older cycle (e.g., Cycle 0), completely ignoring the new information provided by the user in the current cycle (e.g., Cycle 1). Additionally, the UI fails to automatically navigate to the new cycle after generation is complete.
-   **Root Cause Analysis (RCA):** This is a critical workflow failure with two main causes:
    1.  **Stale Data:** The `prompt.service.ts` was not correctly combining the fresh, in-memory `cycleData` (for Cycle `N`) sent from the frontend with the full history read from disk. A logic error caused it to ignore the new `cycleContext` and build the prompt using only the older, persisted data for Cycle `N`.
    2.  **Navigation Failure:** The frontend's message handler for `SendBatchGenerationComplete` was not reliably triggering the navigation to the new cycle (`N+1`), leaving the user on the old cycle view.
-   **Codified Solution & Best Practice:**
    1.  The backend handler for `RequestNewCycleAndGenerate` must use the `cycleData` from the client as the absolute source of truth for the current cycle.
    2.  The `prompt.service.ts` must ensure its logic correctly overwrites any stale data from the history file with the fresh data from the client before assembling the prompt.
    3.  As a best practice for transparency, the generated prompt string should be written to `prompt.md` in the workspace before being sent to the LLM.
    4.  The frontend's `SendBatchGenerationComplete` handler must reliably call the navigation logic to switch the view to the `newCycleId`.

---

### Case Study 039: vLLM Responses Truncated at Stop Token

-   **Artifacts Affected:** `A90. AI Ascent - server.ts (Reference).md`, `src/backend/services/llm.service.ts`
-   **Cycles Observed:** C44
-   **Symptom:** When generating batch responses from the vLLM server, the AI-generated text is cut off prematurely, often right before it would have written `